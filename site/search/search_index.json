{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TLDR","text":"<p>Two decoupled parts - observability and evaluation.</p>"},{"location":"#observability","title":"Observability","text":"<ol> <li>Observability by tracing/logging out data.</li> <li>Performing evals on these datasets.</li> </ol> <p>Agents/tools are (mathematically) a function with:</p> <ul> <li>input</li> <li>output</li> <li>context (optionally)</li> <li>metadata (model, temperature, etc.)</li> </ul> <p>Each Agent has its own self-reflection by exporting the above to a CSV.</p> <p>This CSV can then be analysed using a range of Eval Libraries like Evidently, DeepEval and Ragas, with or without references (ground truths).</p> <p></p> <p>We find where the LLM point is in our code if we are doing evals after dev:</p> <p></p>"},{"location":"#evaluation","title":"Evaluation","text":"<p>Add REFERENCE column to dataset and then do evals.</p> <p>Once we have our output dataset, we can add ground truths/references and then carry out evals using a range of libraries like Evidently.ai, (my favourite), RAGAS, DeepEval etc.</p> <p>We can also create our own evals.</p> <p>The log files are generated in real time enabling not just evaluation but also real time monitoring.</p> <p>I have also found the log files very useful during development work and bug-fixing.</p>"},{"location":"#advantages","title":"Advantages","text":"<p>Decouples app code and evaluation code.</p> <p>Portable Agent/Eval combinations.</p> <p>Evals are real time and support both developer and QA.</p> <p>Frictionless integration to existing apps and new ones.</p> <p>CSV output easier to digest than UI traces. In fact, I tend to want to export the traces as CSVs.</p>"},{"location":"about/","title":"Evaluating AI Agents","text":""},{"location":"about/#aim","title":"Aim","text":"<p>This is a manual to establish a fricitonless way of testing and evaluating Agentic systems both for the developer and QA.</p>"},{"location":"about/#agentic_evaluations","title":"Agentic Evaluations","text":"<p>A Unit Test in agentic terms is the smallest block of code that uses an llm to determine the ROUTE and the RESPONSE.</p> <p>It may contain other deterministic functionality which we can test in the usual way, but this manual focusses on testing and monitoring Agentic systems.</p>"},{"location":"about/#app","title":"App","text":"<p>The app is based on Langgraph which is able to produce an image of the workflow as it is a graph based application.</p> <p></p> <p>The 'should_write' agent decides of an article is football related, and if so an article is written based on the healdine given.</p> <pre><code>\"\"\"You are a grader assessing whether a news article concerns a football transfer. \\n\n    Check if the article explicitly mentions player transfers between clubs, potential transfers, or confirmed transfers. \\n\n    Provide a binary score 'yes' or 'no' to indicate whether the news is about a football transfer.\"\"\"\n</code></pre> <p>The editor then supervises two agents:</p> <ol> <li>An agent that translates \"You are a translator converting articles into German. Translate the text accurately while maintaining the original tone and style.\"</li> <li>An agent that expands an article if too short \"You are a writer tasked with expanding the given article to at least 200 words while maintaining relevance, coherence, and the original tone.\"</li> </ol> <p>If it feels these two criteria are met it passes the article to the publisher agent.</p> <p>The editor agent then decides if it is in German, of correct length, sensational enough and if so moves to the publishing agent where it is published and the workflow comes to and END.</p>"},{"location":"about/#patterns","title":"Patterns","text":""},{"location":"about/#routing","title":"Routing","text":"<p>One fundamental pattern is ROUTING - does the Agent select the correct tool/function/skill with the correct inputs?</p> <p><code>src\\article_writer_langgraph.py</code> shows the routing pattern for <code>system_grader</code> and the log output is in <code>article_writer.csv</code>:</p> <p></p> <p><code>ArticlePostabilityGrader</code> logs to <code>article_writer_can_publish.csv</code> (we can have just one log file):</p> <p></p> <p>In general:   </p> <p>There is also NEXT - does the Agent select the correct next step where this is applicable?</p> <p></p>"},{"location":"about/#output","title":"Output","text":"<p>For a given input, we will obtain an output. </p> <p>We may retrieve additonal context to support the generation of the output.</p> <p>We will also have REFERENCES - ground truths.</p> <p>Our goal is to get a number of datasets:</p> <p>INPUT - OUTPUT - CONTEXT - REFERENCE</p> <p>TOOL_CALLED - ARGUMENTS - NEXT - EXPECTED</p> <p>Once we have these there are many libraries or our own custom evaluations that we can use.</p> <p>We will have a confusion matrix (context may or not exist):</p> <p>INPUT - OUTPUT - CONTEXT - REFERENCE</p> <p>We can then work out an evaluation.</p> <p>We look for OMISSIONS - ADDITIONS - CONTRADICTIONS - COMPLETENESS as alternatives to traditonal F1 scores although these can be computed as well.</p> <p>We can also evaluate system fails:</p> <p></p>"},{"location":"about/#frictionless","title":"Frictionless","text":"<p>The process needs to be frictionless for developers.</p> <p>At an accessability talk, the speaker said 'How do you make a blueberry muffin?'. </p> <p>You put the blueberries in at the beginning and not stuff them in at the end.</p> <p>This can be done either with supplied decorators as in <code>openai_agent_logging.py</code> for tool calling or by supplying the appropriate tracing function as in article_writer_langgraph.ipynb`:</p> <p><pre><code>##############################################\n#\n# This can be standardised during development\n# DATE|COMPONENT_CODE|MODEL|TEMPERATURE|INPUT|OUTPUT and any optional fields\n#\nwith open(\"article_writer.csv\", \"a\", encoding=\"utf-8\") as f:\n    f.write(\n        f\"{report_date}|EVALUATOR|{MODEL}|{TEMPERATURE}|{INPUT}|{OUTPUT.binary_score}\\n\"\n    )\n##############################################\n</code></pre> The core tracing is for datetime|component|model|temperature|input|output</p> <p>Additional data can be added optionally in a structured way.</p> <p>This serves as testing, evaluationg and monitoring from development to production.</p> <p>The developer and QA work towards making each UNIT fully introspective.</p> <p>Having researched the various Observability tools, I think this logging approach is the simplest and possibly most effective as it separates data collection and data evaluation, and also avoids being tied to any framework.</p>"},{"location":"agent_raw/","title":"A raw Agent","text":""},{"location":"agent_raw/#what_is_an_agent","title":"What is an Agent?","text":""},{"location":"agent_raw/#api","title":"API","text":""},{"location":"agent_raw/#self_reporting","title":"Self Reporting","text":""},{"location":"agent_raw/#building","title":"Building","text":"<p>We will build an AI Agent from scratch to see how to include self-onservability.</p>"},{"location":"preamble/","title":"The Agentic Paradigm","text":"<p>Traditionally, testing is focussed on ensuring the pipelines of the factory works correctly.</p> <p>It does not focus on the quality of content produced.</p> <p>Logging/tracing, rather than unit and integration tests, are of more value here. It can often be a challenge to create tests for Agentic Systems based on agentic pipelines being different to tradional ones.</p> <p>Below we can see how tracing/logging is part of the development process that will assist the developer and end user.</p> <p> We test, evaluate and monitor to ensure our client's needs and wants are met.</p>"},{"location":"preamble/#evaluation_driven_development","title":"Evaluation Driven Development","text":"<p>If we consider our whole app to be a molecule based on a number of atoms, we use EDD to build in evaluations so that development and production can benefit from immediate feedback.</p>"},{"location":"case_study/about/","title":"Case Studies","text":""},{"location":"case_study/about/#first_steps","title":"First steps","text":"<p>We need to understand what the app does and what the workflow is.</p> <p>We identify the Agentic points where we use LLMs and then apply logging.</p> <ul> <li>What does a satisified user look like?</li> <li>What would a disappointed user look like?</li> <li>How do we define a successful app from our point of view?</li> </ul> <p>We need to make each agent self-evaluating. That is, we need to export to CSV input|output|context|tool_use|metadata.</p> <p>Then we can add references if we have them and run our evals.</p> <p>We will do this manually to get a sense of what we need to implement and if possible scale up with automations and LLM as judge.</p>"},{"location":"case_study/about/#case_study_1","title":"Case Study 1","text":""},{"location":"case_study/about/#langgraph","title":"Langgraph","text":"<p>In this first case study, we use a sample Langgraph app located in <code>src/case_study1/langgraph/app_article_writer.py</code></p> <p>Evals where done after the app was built so we idendify the Agentic points where we use LLMs and then apply logging.</p> <p>There are 4 LLM instances and this is where we do our Agentic Evals.</p> <p>EVAL01:</p> <p></p> <p>We log to <code>01_article_writer_should_write.csv</code> and do the same for the other 3 evals.</p>"},{"location":"case_study/about/#case_study_2","title":"Case Study 2","text":""},{"location":"case_study/about/#openai_agents","title":"OpenAI Agents","text":"<p>This is a case study using a Deep Research clone app by Kody Simpson.</p> <p>I coded it up without the 'Follow up' on research items.</p> <p>The repo is https://github.com/Python-Test-Engineer/llm-evaluation-framework</p> <p>Located in src/case_study2/deep_research it uses the OpenAI Agent SDK.</p> <p>It enables us to use an actual app to set up Evals.</p> <p><code>uv run ./src/deep_research/main.py</code></p>"},{"location":"case_study/about/#evaluating","title":"Evaluating","text":"<p>With the app is we will go through a generic sequence of how we carry out evals:</p> <ol> <li>What does the end user want?</li> <li>How can we evaluate and monitor to ensure our client's needs and wants are met?</li> <li>What and where do we log?</li> </ol>"},{"location":"case_study/about/#case_study_3","title":"Case Study 3","text":""},{"location":"case_study/about/#tool_calling","title":"Tool calling","text":"<p>We have a separate case study for tool calling to avoid cluttering the other case studies.</p> <p>We want the end branch of our app where we have the lass Agentic component.</p> <p>We can then combine all case studies as sub graphs.</p>"},{"location":"case_study/about/#evaluations","title":"Evaluations","text":"<p>For tool calling we want to evaluate:</p> <ol> <li>Was the right tool called for the right input?</li> <li>Were the arguments correct?</li> <li>Did the tool produce the right output?</li> </ol> <p>If the tool uses another agentic function the we would evaluate that subgraph but in our case study we are not using any agentic subgraphs and any functions used would be deterministic in nature.</p>"},{"location":"case_study/case_study1/","title":"Case Study 1","text":""},{"location":"case_study/case_study1/#article_writer","title":"Article Writer","text":"<p>This is a leaf level sub graph meaning it is at the end of all the Agentic actions.</p> <p>The first node <code>should_write_article</code> determines if the short article headline is about the selected content topic, in our case AI/Technology.</p> <p></p> <p>If so, it goes to the editor that ensure it is translated into the chosen language, French, and has sufficient length and is not sensational. </p> <p>This then passes to the <code>publisher</code> and if all are yes, it set <code>postability</code> to yes and in the case ends there, though in production some actions would be taken.</p> <p>As you can see from the output, the app works through each of these processes and if all are <code>yes</code> then postability is set to <code>yes</code> and we move to <code>__END__</code>.</p> <p></p> <p>This case study shows how we can evaluate ROUTING and also parallel tool call - we can think of a node as a tool call as everything is just a function.</p> <p>If we had an action at the publisher node, we could then verify that articles are only published if all thre criteria are <code>yes</code>. We could do this by setting a <code>is_published=yes</code> and we can then check deterministically that if <code>is_published=yes</code> then all the others should also be yes.</p> <p>We can dump all the output in the log file so that we can see more detail about the LLM call like tokens etc. or filter before dumping to log file.</p>"},{"location":"case_study/case_study2/","title":"Case Study 2","text":""},{"location":"case_study/case_study2/#the_app","title":"The App","text":"<p>This case study is about a deep research agent that creates a report based on a user query.</p> <ol> <li>Create N (configurable) questions to ask.</li> <li>Create X (configurable) search queries for each question.</li> <li>Use a SYNTHESISER agent to create the final report.</li> </ol> <p>The coodrdinator.py has access to 4 agents, (follow_up_agent.py is not used).</p> <p></p>"},{"location":"case_study/case_study2/#evals","title":"Evals","text":"<p>We need to determine what our top level evals are:</p> <p>For a given question, we get a report that is about the question, has the required length and answers the question. We can check length deterministically, we can check the quality firstly by human and then build our LLM judge. \"A useful report\"</p> <p>We can have evals for the steps to make the report:</p> <ol> <li>Check N and X config values were followed.</li> <li>Evaluate the relevance of questions and queries in relation to the question.</li> <li>Did the article and retrieved content perform well on standard RAG evaluations - see Evals section.</li> </ol>"},{"location":"case_study/case_study2/#tracing","title":"Tracing","text":"<p>TODO: Add these evals and discuss here...</p>"},{"location":"case_study/case_study3/","title":"Case Study 3","text":""},{"location":"case_study/case_study3/#tool_calling","title":"Tool calling","text":"<p>The module <code>tool_calling.py</code> has 4 tools and they form pairs to test multiple tool calling in a both parallel and sequential modes.</p> <p>I have tried to muddy the water regarding weather and temperature so that it can stress tool choice.</p>"},{"location":"case_study/case_study3/#independent_tools","title":"Independent tools","text":"<p>Parallel or independent tool calling example:</p> <p>TOOL 1: - get weather (fake api) gets weather for a city.</p> <p>TOOL 2: - Checks for seating availablity</p> <p>These two answer the question: 'What is the temperature in Munich and are there seats indoors/outdoors available'</p>"},{"location":"case_study/case_study3/#dependent_tools","title":"Dependent tools","text":"<p>Sequential or dependent tool calling example:</p> <p>TOOL 3: - Converts Centigrade to Fahrenhiet</p> <p>TOOL 4: - For a given temperature in F, gives a lable of COLD, MILD, WARM or HOT.</p> <p>Tools 3 and 4 are sequential in answering the question 'What is 12C in Fahrenheit and give me the label/description of that temperature'.</p> <p>Of course, this could be just one tool but this is a demo of sequential tool calling and checking correct tools and arguments are used as well as giving the correct outputs.</p>"},{"location":"case_study/case_study4/","title":"SQL Agent","text":""},{"location":"case_study/case_study4/#app","title":"App","text":"<p>We will evaluate the SQL Agent from Langchain as the basis for a chain of SQL in an Agentic app. </p> <p>This is a full stack app with detailed video by Langchain using the repo and article below.</p> <p>https://www.youtube.com/watch?v=LRcjlXL9hPA</p> <p>https://github.com/DhruvAtreja/datavisualization_langgraph/tree/main</p> <p>https://blog.langchain.com/data-viz-agent/</p>"},{"location":"case_study/case_study4/#eval","title":"Eval","text":"<p>We can consolidate the principles we have been developing in this manual:</p> <ol> <li>Find the Agentic inflection points.</li> <li>Log out INPUT-CONTEXT-OUTPUT-METADATA</li> </ol> <p>The app has a central LLM Manager (https://github.com/DhruvAtreja/datavisualization_langgraph/blob/main/backend_py/my_agent/LLMManager.py</p> <p></p> <p>It is tempting to use this one LLM inflection point to do our logging.</p> <p>However, a fundamental principle is that each agent contains its own observability capability and we would be vulnerable to changes/removal of the LLM Manger.</p> <p>This we will look throuhg the https://github.com/DhruvAtreja/datavisualization_langgraph/tree/main/backend_py/my_agent folder and find the LLM calls.</p> <p>We will then add our logging, customised to the agent, and logged to its own logging file.</p> <p>We can always merge log files later if needed.</p> <p></p> <p>Here we can see the SQL Agent https://github.com/DhruvAtreja/datavisualization_langgraph/blob/main/backend_py/my_agent/SQLAgent.py and the INPUTS of <code>prompt, schema and question</code> along with the OUTPUT of <code>parsed_response</code> can be logged along with all the other metadata from then input and output side as needed.</p> <p>We can also log the STATE machine at any point too.</p> <p>Again, it ends up being the insertion of a log statement after any LLM call:</p> <p></p> <p>with its exact structure varying with the agent and being logged to its own associated log file.</p>"},{"location":"case_study/evidentlyai/","title":"Evidently AI","text":"<p>I liked the straight forwardness and ease of use of Evidently.ai.</p> <p>It does not need to be coupled with the app but can work on datasets that have been outputed. We will just need to add references to the output files as appropriate.</p> <p>There is a very good video series here: </p> <p>https://www.youtube.com/watch?v=jQgI8tTkWQU&amp;list=PL9omX6impEuNTr0KGLChHwhvN-q3ZF12d&amp;index=2.</p> <p>Along with great notebooks here: </p> <p>https://github.com/evidentlyai/community-examples/tree/main/learn </p> <p>We will now look at these notebooks using outputs from our case studies.</p> <p>...</p>"},{"location":"craig/contact/","title":"Contact me","text":"<p>Email: iwswordpress@gmail.com</p> <p>LinkedIn: Craig West</p>"},{"location":"craig/courses/","title":"Online courses","text":""},{"location":"craig/courses/#udemycom","title":"Udemy.com","text":"<p>The course Udemy Hooks and Plugins course has just been published and Udemy has a sale ever two weeks and the cost would be $20 USD approx.</p> <p></p> <p>I am currently developing two courses:</p> <ul> <li>Python - mock, patch and monkeypatch.</li> <li>PyTest Django Full Stack - a DB &lt;-&gt; E2E testing of a generic ecommerce store.</li> </ul> <p>The aim is to make them generic, ready to go templates, that also dive deeper into aspects of Python.</p> <p>I am of the opinion that as developers we do not need to reinvent the wheel - it has (almost) all been done before - and that we should be free to use our creativity to build great proucts.</p> <p>The type of course I would want...</p> <p></p>"},{"location":"craig/cv/","title":"Cv","text":""},{"location":"craig/cv/#website","title":"Website","text":"<p>https://craig-west.netlify.app/</p>"},{"location":"craig/cv/#github_cv","title":"GitHub CV","text":"<p>I use GitHub to host a copy of my CV.</p> <p>Github CV</p>"},{"location":"craig/cv/#python_backend_and_test_automation_engineer","title":"Python Backend and Test Automation Engineer","text":"<ul> <li>Degree in Chemistry, Oxford University.</li> <li>Former A+ PC Technician, Microsoft Certified Systems Engineer and Microsoft Certified SQL Server DBA.</li> <li>Former Business Information Architect.</li> <li>Qualified Accountant Technician and business owner.</li> <li>Experience with REST APIs, GraphQL, React, Vue, Web Components, Node, Docker</li> <li>Talks and workshops given at WordCamps, MeetUps and NDC.</li> </ul>"},{"location":"craig/cv/#talks_and_workshops","title":"Talks and Workshops","text":""},{"location":"craig/cv/#python","title":"Python","text":"<ul> <li> <p>BrightonPy Feb 2025: AI as API in everyday Python apps - 60 minute talk and demo.</p> </li> <li> <p>Django Japan Congress Feb 2025 (online): Implementing Agentic AI solutions in Django from scratch - 45 minute talk</p> </li> <li> <p>Conf42 (online) Feb 2025: Implementing Agentic AI Solutions in Python from scratch - 50 minute talk.</p> </li> <li> <p>PyCon Ireland Nov 2024: Getting started with Pytest - 2 hr workshop.</p> </li> </ul> <p>Next...</p> <ul> <li>DjangoConEurope April 2025: Implementing Agentic AI solutions in Django from scratch - 90 minute workshop.</li> </ul>"},{"location":"craig/cv/#other","title":"Other","text":"<ul> <li> <p>TALK: Offline and instant websites, aka Progressive Web Apps - AsyncJS, Brighton, September 2021.</p> </li> <li> <p>LIGHTNING TALK: WordPress as a Micro Service to any framework - WordFest, July 2021.</p> </li> <li> <p>TALK: WP REST API and Web Components =&gt; 100% Internet - WordCamp Santa Clarita, July 2021.</p> </li> <li> <p>TALK: Web Components in WP, Gutenberg and as HTML plugins. - WordCamp North East Ohio May 2021.</p> </li> <li> <p>TALK: Leveraging the power or the WordPress REST API - WP Leeds April 2021</p> </li> <li> <p>WORKSHOP: WP REST API and you -&gt; Best Friends Forever workshop (90 mins) - WordCamp Greece April 2021. Code: https://github.com/iwswordpress/WordCampGreece</p> </li> <li> <p>TALK: Web Components as Micro Apps - NDC London, Jan 2021</p> </li> <li> <p>TALK: Unifying frameworks with Web Components - Brighton AsyncJS, Nov 2020.</p> </li> <li> <p>WORKSHOP: Progressive Web Apps Workshop (2hrs) - NDC Oslo June 2020 and a paid training workshop with NDC.</p> </li> <li> <p>WORKSHOP: Web Components Workshop (2hrs) - NDC Oslo June 2020 and a paid training workshope with NDC.</p> </li> <li> <p>WORKSHOP: Progressive Web Apps Workshop (2hrs) - Brighton WordUp June 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API with AJAX Forms and Pages - WordCamp Denver, June 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API with AJAX Forms and Pages - WordCamp Kent, Ohio May 2020.</p> </li> <li> <p>TALK: What is the WP REST API and how can I use it to make forms and pages that don\u2019t need to do be reloaded? - WordUp Brighton May 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API and AJAX Forms - WordCamp Geneva March 2020 [EVENT CANCELLED due to virus concerns :( </p> </li> <li> <p>TALK - WP-HTML: The marriage of WP and JS Frameworks for expansion, ubiquity and profit - WordCamp Vienna February 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API - WordCamp Vienna February 2020.</p> </li> <li> <p>TALK: Progressive Web Apps - Brighton WordUp November 2019.</p> </li> <li> <p>TALK: Decoupled WordPress (code along style) - WordCamp Dublin October 2019.</p> </li> <li> <p>TALK: JWT and Authentication - WPHooked London September 2019.</p> </li> <li> <p>TALK: Decoupled WordPress and WP Components - WordCamp Brighton August 2019.</p> </li> </ul>"},{"location":"craig/cv/#published_udemy_courses","title":"Published Udemy Courses","text":"<p>Udemy is a great learning platform and having sales at least once a month, courses can be purchased for ~ \u00a315/$15 USD.</p> <p>These have now been retired.</p> <ul> <li>WordPress REST API and AJAX Forms/Pages - DEMO https://www.youtube.com/watch?v=eubhbcGH_Ws&amp;t=6s (paid)</li> <li>Progressive Web Apps - DEMO https://www.youtube.com/watch?v=k_lHvNL0gkw (paid)</li> <li>WP-HTML: decoupling WordPress to any HTML platform using Web Components and the WP REST API. This also enables HTML plugins for non-WP Sites - https://www.udemy.com/course/powerful-html-pages-using-wordpress-component-architecture/ (free)</li> <li>Stylish Dynamic Web Forms with jQuery validation - https://www.udemy.com/course/ready-to-use-form-validation-templates-with-jquery/ (free)</li> </ul>"},{"location":"craig/cv/#youtube_courses_-_developer_to_developer_courses","title":"YouTube Courses - Developer to Developer courses","text":"<p>These are video courses that cover work through official documents to help other developers, learn in public and show prosepective employers not just what I know but how I learn and how I communicate technical matters to others.</p> <p>There are also some specific videos explaining solutions to set ups other developers may encounter.</p> <ul> <li> <p>HIGHLY-FUNCTIONAL-WEBCOMPONENTS: A video course based on the workshop I gave at NDC Oslo June 2020 - https://www.youtube.com/watch?v=QC-JTqQTv2k&amp;list=PLsszRSbzjyvkQwzrJobroRl7z7MfSlePa </p> </li> <li> <p>WP Plugin Boilerplate:  I havea video series to explain WP Plugin Boilerplate using a scaffolded out project that demonstrates the use of MySQL, wp_nonce, REST API, forms and how to redirect pages to plugin templates to make the plugin theme independent. https://www.youtube.com/watch?v=lJ9ktD4JOfs&amp;list=PLsszRSbzjyvn-RQr4dEjrgnTne2HcJKee</p> </li> </ul>"},{"location":"craig/cv/#volunteering","title":"Volunteering","text":"<p>I volunteer at Codebar.io in Brighton as well as some Community Kitchens.</p>"},{"location":"craig/cv/#outside_interests","title":"Outside interests","text":"<p>These include Community Kitchens, gym, occasional partner dancing and DIY.</p> <p></p>"},{"location":"craig/services/","title":"Services provided","text":""},{"location":"craig/services/#backend_pythonista_and_test_automation_engineer","title":"Backend Pythonista and Test Automation Engineer","text":""},{"location":"craig/services/#skillset","title":"Skillset","text":"<p>Primarily:</p> <ul> <li>Python</li> <li>PyTest</li> <li>Playwright</li> <li>Django</li> </ul> <p>Tools:</p> <p>I strive to dive deeper into these tools and see them as programming languages in their own right. DevOps seems to be an essential part of my work:</p> <ul> <li>Shell Scripting</li> <li>Git/GitHub Actions</li> <li>Docker</li> </ul>"},{"location":"craig/services/#engagement_style","title":"Engagement Style","text":"<p>I offer on-demand, freelance services starting from 1/2 day blocks.</p> <p>As and when you need it...</p> <p>Tech is a way of life for me not just a job and I strive to have enthusiasm and passion for the projects I work on. Professional fulfilment is paramount.</p>"},{"location":"craig/services/#eligibility","title":"Eligibility","text":"<ul> <li>UK National</li> <li>Fluent English</li> </ul>"},{"location":"craig/services/#on-sitehybrid","title":"On-site/Hybrid","text":"<p>I am based in Brighton and enjoy (local) on-site work as well as working from my home office.</p>"},{"location":"craig/services/#volunteer_coach","title":"Volunteer Coach","text":"<p>I am a volunteer coach with Codebar Brighton.</p>"},{"location":"craig/services/#youtube","title":"YouTube","text":"<p>I produce a large amount of content that is associated with a repo that enables 'out of the box' ease of use.</p> <p>If I find good videos without a repo, I often create a repo and my own video with reference to the source video. I have no commercial interest in this matter.</p> <p>My YouTube Channel</p>"},{"location":"craig/services/#outside_of_tech","title":"Outside of tech...","text":"<p>I enjoy working in community kitchens and love laughter, creating, doing and trying to work out why things are funny.</p> <p></p>"},{"location":"evaluation/domain_expert/","title":"Professional Evaluation","text":"<p>The technique of an (untested) LLm generating question/answer pairs to act as ground truths to then test an LLM seems illogical but current thinking states this is quite effective.</p> <p>At the end of the day, the final judge of the LLM is a human. </p> <p>A number of sets of questions can be made and the LLM evaluated against them by a domain expert.</p> <p>Whilst a numerical rating system is the immediate choice, there are many flaws with this.</p> <p>The domain expert can rate the repsonse for the following:</p> <ol> <li>Accuracy. Is the answer factually correct?</li> <li>Relevance. Is the answer relevant to the question?</li> <li>Completeness. Is the answer complete?</li> <li>Clarity. Is the answer clear?</li> </ol> <p>Only those that get a YES to ACCURACY and RELEVANCE will be considered as they are essential for a good response.</p> <p>This is a very effective way of evaluating the LLM and will help us to improve our system and will be modiofied in the future as needed.</p>"},{"location":"evaluation/domain_expert/#example","title":"Example","text":""},{"location":"evaluation/eval_framework/","title":"Eval Framework","text":"<p>I have combined PyTest Full Stack with my sample app and tests to create an evaluation framework.</p> <p>https://github.com/Python-Test-Engineer/llm-evaluation-framework</p>"},{"location":"evaluation/evidently/","title":"Evidently.ai","text":"<p>I have found Evidently to be very user friendly with great videos and example code.</p> <p>It works on dataset rather than being placed in source code.</p> <p>There are a range of notebooks that work iwth an OpenAI key and they are in the <code>src/evidently</code> folder:</p> <p></p> <p>One can see from the file names the range of evals it does.</p> <p>The notebooks are documented and runnable. They are part of the video series Evidently.</p>"},{"location":"evaluation/libraries/","title":"Libraries","text":"<p>There are a number of ways we can evaluate the performance of the app. </p> <p>These use traditonal Natural Language Processing (NLP) and Machine Learning (ML) techniques.</p> <p>There are a number of frameworks for evaluations.</p> <p>Evidently.ai and RAGAS are two that I like to work with.</p>"},{"location":"evaluation/libraries/#evidentlyai","title":"Evidently.ai","text":"<p>https://github.com/evidentlyai/community-examples</p> <p>I find this broader than RAGAS although one can tailor RAGAS in the same way.</p>"},{"location":"evaluation/libraries/#ragas_metrics","title":"Ragas metrics","text":"<p>https://docs.ragas.io/en/stable/</p> <p>This is much more RAG focused.</p>"},{"location":"evaluation/libraries/#context_precision","title":"Context Precision","text":"<p>This is a metric that measures the proportion of relevant chunks in the retrieved_contexts. It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.</p>"},{"location":"evaluation/libraries/#context_recall","title":"Context Recall","text":"<p>Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out. In short, recall is about not missing anything important. Since it is about not missing anything, calculating context recall always requires a reference to compare against.</p>"},{"location":"evaluation/libraries/#faithfulness","title":"Faithfulness","text":"<p>Faithfulness metric measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.</p> <p>The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context. To calculate this, a set of claims from the generated answer is first identified. Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context. </p>"},{"location":"evaluation/libraries/#other_evaluations","title":"Other evaluations","text":"<p>There are many libraries for evaluating the perfromance of our app, like Giskard and Huggingface.</p>"},{"location":"evaluation/libraries/#domain_expert_evaluation","title":"Domain Expert Evaluation","text":"<p>Ultimately, the real test is that of the domain expert.</p> <p>Here, we have a set of questions and answers and we want to evaluate the performance of the app.</p> <p>We can create a set of 'ground truths', questions and their correct answer set up by domain experts.</p> <p>We can then have our app generate answers and context to these questions for a domain expert to assess, not just the accuracy but also whether the context returned was relevant:</p> <p></p>"},{"location":"evaluation/llm_ml_nlp_jury/","title":"Jury","text":"<p>We can have a panel of judges to create a final decision in one of many ways or use a Final Judge that reviews all the opinions of judges and forms a final decision.</p> <p>We can use ML and NLP as judges and not just LLMs.</p> <p>Building specialist judges to rate and give reason for various qualitative metrics like bias, conciseness, usefulness etc can create reusable judges.</p>"},{"location":"evaluation/overview/","title":"Overview","text":"<p>\"How do you make a blueberry muffin? You put the blueberries in at start not stuff them in at end.\" - Accessibility speaker.</p> <p>It is part of the development lifecycle supporting the developer and all othe rstake holders.</p> <p></p> <p>It is most useful to ask stakeholders what is important to them and how they will assess the effectivenes of the Agent.</p>"},{"location":"evaluation/overview/#evaluations","title":"Evaluations","text":"<ol> <li> <p>Code based - traditional testing</p> </li> <li> <p>LLM as Judge - specialist LLMs to rate tone, completeness, no PII, matrix analysis, gender bias etc as well as the confusion matrix of input-output-context-reference.</p> </li> <li> <p>ML analysis - we may use ML models to compute a metric.</p> </li> <li> <p>Human Annotations.</p> </li> </ol> <p>Numerical evals are hard for LLMs so we use more qualitative measures.</p> <p>Start with Human Evals and scale with LLM Judge. This seems the top recommendation from those in the field.</p> <p>Human evals will reveal what we need to test.</p> <p>LLM Judge will enable scaling - we ask the LLM for its reasoning for its grade. </p> <p>The experts say this will reveal many things, notably whether the LLM and us are not on the same page in the evaluation process. We can then change the prompt.</p> <p>Getting judge to give its reasoning is vital to check judging is on the mark.</p>"},{"location":"evaluation/overview/#scoping","title":"Scoping","text":"<ul> <li>Trace - the run</li> <li>Span - items within the run</li> </ul>"},{"location":"evaluation/overview/#telemetry","title":"Telemetry","text":"<p>CORE DATA:</p> <ul> <li>Some of these are optional</li> <li> <p>We may use a code to identify the unit under test, e.g <code>app_unit_test_type</code></p> </li> <li> <p>RUN_ID (unique run id to group all traces in a workflow)</p> </li> <li>USER_ID</li> <li>ENVIRONMENT (dev/staging/prod)</li> <li>DATETIME</li> <li>TRACE</li> <li>SPAN</li> <li>MODEL</li> <li>MODEL_KWARGS (temperature etc)</li> <li>INPUT</li> <li>OUTPUT</li> <li>CONTEXT</li> <li>TOOL_CALL</li> <li>TOOL_INPUT</li> <li>TOOL_OUTPUT</li> </ul> <p>(custom data) - WHAT_DEV_ADDS</p> <p>These CSVs are appropriately named and will then have REFERENCE ground truth added to provide an dataset for analysis.</p> <p>We can also run evals with no references. We use LLM as judge to determine the eval.</p> <p>There are a number of libraries I like:</p> <ol> <li>Evidently AI - favourite of mine as it made Evals easy and enjoyable.</li> <li>Deep Eval</li> <li>RAGAS</li> </ol>"},{"location":"evaluation/overview/#llm_as_jury","title":"LLM as Jury","text":"<p>LLM as a jury involves using large language models to simulate jury decision-making processes, evaluating evidence and reaching verdicts like human juries, though with significant limitations around bias and moral reasoning.</p>"},{"location":"evaluation/overview/#common_mistakes","title":"Common mistakes","text":""},{"location":"evaluation/pytest_full_stack/","title":"PyTest Full Stack","text":"<p>When I was studying/wroking with PyTest, I came across a great many resources, yet each was not complete.</p> <p>I put together a template for PyTest Full Stack that enabled developers to drop thier app in <code>src</code> and have a complete PyTest Full Stack suite to test from DB schema all the way through API and E2E testing.</p> <p>It contains around 200 template tests from DB Schema to API and E2E browser tests that are ready to go.</p> <p>I published the manual at https://pytest-cookbook.com/ and the repo at https://github.com/Python-Test-Engineer/PyTest-Full-Stack.</p> <p>It had HTML, Coverage and its own CSV custom reporting built in as well as some plugins for console formatting.</p> <p>I gave a workshop 'Getting started with PyTest' at PyCon Ireland in November 2024.</p> <p></p> <p></p> <p>This can then be used as a full QA testing of an application.</p>"},{"location":"evaluation/rate_with_llm/","title":"LLM as judge","text":"<p>The technique of an (untested) LLm generating question/answer pairs to act as ground truths to then test an LLM seems illogical but current thinking states this is quite effective.</p> <p>This technique might be used by developers to improve the knowledge system as they work on it rather than getting human evaluations at every step.</p> <p>This is more of a development tool rather than final evalauation.</p> <p>Arxiv Paper: https://arxiv.org/pdf/2412.05579</p> <p>Definitive guide to building LLM Judges</p> <p></p>"},{"location":"evaluation/rate_with_llm/#uses","title":"Uses","text":"<p>Some example additional uses:</p> <ul> <li>Politeness: Is the response respectful and considerate?</li> <li>Bias: Does the response show prejudice towards a particular group?</li> <li>Tone: Is the tone formal, friendly, or conversational?</li> <li>Sentiment: is the emotion expressed in the text positive, negative or neutral?</li> <li>Hallucinations: Does this response stick to the provided context?</li> <li>Adversarial: We can test it does NOT do things as well as test edge cases.</li> </ul> <p>By asking for not just the grade but its reasoning, we can get a more complete picture of how the judge is evaluating the LLM.</p> <p>Code demo:</p> <p></p> <p>Examples</p> <p>https://www.youtube.com/watch?v=LZJTrAXcyFM</p> <p></p> <p></p> <p></p>"}]}