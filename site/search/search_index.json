{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AI Powered Knowledge Systems","text":"<p>This is a work in progress where I collect resources and ideas concerning AI Search, Agentic AI and AI Data Analyst.</p> <p>As others have shared with me, I share my own research.</p> <p>The areas I am working on are:</p>"},{"location":"#ai_search","title":"AI Search","text":"<p>Using both Semantic and traditonal keyword search, a Hybrid search system can yield the best document retrieval for RAG implementations, particularly when combined with Agentic RAG strategies.</p> <p>We get the best k results from Full Text Search/Keyword search and the k best results from semantic search, combine them together, rerank and then return N best results.</p>"},{"location":"#agentic_rag","title":"Agentic RAG","text":"<p>Retrieval Augmented Generation enables us to add domain specific data to our our prompts for better answers.</p> <p>Agentig RAG uses Agents to plan, retrieve, refine and generate answers using specilaist agents for each of these aspects.</p> <p>An example of this is Langgraph's agentic-RAG:</p> <p></p>"},{"location":"#ai_data_analyst","title":"AI Data Analyst","text":"<p>The use of AI Agents in the Data Pipeline from ETL, Analysis to Reporting enables us to generate reports from data sources.</p> <p>This is the emerge of the AI Data Analyst.</p> <p>Some aspects are best done with traditional Data Processing and we should not make them agentic just because that is in fashion.</p> <p>This can add non-deterministic bureaucracy to the data pipeline.</p> <p>There are two main approaches that are detailed in this book - creating SQL queries on the fly and using LLMs to select the most suitable SQL Query that has been developed and tested using AI Search of stored SQL Query Documents that contain not just the SQL but also metadata for report types and charts: </p> <p>https://ai-powered-knowledge-systems.netlify.app/ai_data/#text2sql</p> <p></p>"},{"location":"ai_data/","title":"AI Agents and Data","text":""},{"location":"ai_data/#product_ideas","title":"Product ideas","text":"<p>In developing this workshop, we are developing a product/service where enterprises can 'chat' to not just their data but also to documents and other data sources.</p> <p>We can also develop a DeepResearch where a user's request for an overall report is broken down into parts with each being resolved by SQL, Web Search, Document Search and then collated into a report.</p> <p></p> <p>Using the Python library rpy2 we can also run R in Python.</p> <p></p>"},{"location":"ai_data/#workshop_ideas","title":"Workshop ideas","text":"<p>The exact content and promo description will need to be determined by us to reflect realistically how the workshop will benefit attendees.</p> <p>Would prefer to be labelled as 'D XXXX' rather than Freelancer but that is fine.</p> <p>Over the next 6 months, there will be significant advances in LLM capabilities as well as dramatic cost reductions. </p> <p>The workshop will keep pace with these ideas and be very current with the latest trends.</p> <p>\"When can AI Agents help us in the Data Pipeline and Data Analysis rather than deterministic pipelines that we have already been using successfully for years?\"</p> <ul> <li>2 types of attendees - developer and the data professional that will need to inform their company about AI Agents in the Data Pipeline.</li> </ul>"},{"location":"ai_data/#the_workshop_content","title":"The workshop content:","text":"<ul> <li>Simplification and demystification of AI Agents by building them from scratch using notebooks with embedded slides for any theory that comments don't cover.</li> <li>Examining where the use of AI Agents in the Data Pipeline from ETL, Analysis to Reporting can be of value.</li> <li>Looking at two approaches to SQL queries 1). AI Agents creating the SQL on the fly and 2). Using LLMs to select the most suitable SQL Query that has been developed and tested. For overall DB queries, 2 is favoured.</li> <li>Use of LLMs and Vision models to analyse tabular data and charts etc.</li> <li>Understanding the types of workkflows that can be used and Agentic RAG as well as Multi Agent patterns.</li> <li>How we can synthesise reports to summarise all separate results into one report.</li> <li>Once we understand the raw mechanics we can look at some frameworks available to see how they can assist us further if needed.</li> <li>Examining how we test and evaluate these AI Agents and workflows.</li> <li>Use of Prompting v RAG v Fine Tuning - specialist models for each organisation.</li> </ul>"},{"location":"ai_data/#text2sql","title":"Text2SQL","text":"<p>The core is Text2SQL - when a user asks a question about data, the AI Agent will generate a SQL Query that can be executed to answer the question, along with charts and reports as requested.</p>"},{"location":"ai_data/#llm_on_the_fly","title":"LLM on the fly","text":"<p>AI Agents creating the SQL on the fly and then running them to produce reports. Subject to non-determinism and also being inaccurate which would lose user trust and confidence.</p> <p>Useful as an offering with caveats.</p>"},{"location":"ai_data/#tried_and_tested","title":"Tried and tested","text":"<p>Using LLMs to select the most suitable SQL Query that has been developed and tested. For overall DB queries, I prefer this.</p> <p>This is fundamentally an AI Powered Search of the best 'product'. If we store an SQL query along with metadata, our task is to translate natural language into the best SQL Query that has been tried and tested. This would involve Hybrid Search - both keyword and semantic search.</p> <p>We carry out AI Search to get the best 'product SQL Query Doc' that contains not just the SQL and description but data for report types and charts.</p>"},{"location":"ai_data/#slides","title":"Slides","text":""},{"location":"ai_data/#etl","title":"ETL","text":""},{"location":"ai_data/#planning","title":"Planning","text":""},{"location":"ai_data/#agentic_rag","title":"Agentic RAG","text":"<p>This is just to show how advanced RAG has become rather than the initial 'Naive RAG'.</p> <p></p>"},{"location":"ai_data/#queries","title":"Queries","text":""},{"location":"ai_data/#sql_as_doc","title":"SQL as Doc","text":"<p>It might transpire that have nested sub query docs for a given sql doc may be beneficial.</p>"},{"location":"ai_data/#strategy","title":"Strategy","text":""},{"location":"ai_data/#crowd_sourced_answers","title":"Crowd Sourced Answers","text":"<p>Users can rate the answers along with more metrics:</p> <p></p> <p></p> <p>These then become datasets to use ML to determine user intent for a query as well as optimising selecting the right SQL Doc.</p>"},{"location":"ai_data/#reporting","title":"Reporting","text":"<p>We can create a variety of reports as needed including a Deep Research type report.</p> <p></p>"},{"location":"ai_data/#workflows","title":"Workflows","text":"<p>This is Langgraph buildt by Langchain and uses a Finite State Machine to define the workflow.</p> <p></p> <p></p> <p></p>"},{"location":"books/","title":"Books","text":"<p>I have two other 'books':</p>"},{"location":"books/#pytest_cookbook","title":"PyTest Cookbook","text":"<p>PyTest Cookbook</p>"},{"location":"books/#pytest_django_full_stack","title":"PyTest Django Full Stack","text":"<p>PyTest Django Full Stack</p>"},{"location":"current_concepts/","title":"Current concepts","text":"<p>Let's take a look at some of the current buzz words in this space so that we understand what we are doing.</p>"},{"location":"current_concepts/#llm","title":"LLM","text":"<p>A large language model (LLM) is a type of artificial intelligence (AI) program that can recognize and generate text, among other tasks. LLMs are trained on huge sets of data \u2014 hence the name \"large.\" LLMs are built on machine learning: specifically, a type of neural network called a transformer model.</p> <p>We represent it in our architecture as a brain and this serves us well as we can also think what would a human do at this stage?</p> <p></p>"},{"location":"current_concepts/#semantic_search","title":"Semantic search","text":"<p>Semantic search vectors are numerical representations of data and related contexts that are used to rank and deliver search results based on their relevance. </p> <p>The closer two points in vector space are, the more similar they are.</p> <p></p> <p>OpenAI has 1536 dimensions...</p> <p>There are a few different methods such as cosine similarity, dot product, Euclidean distance etc.</p>"},{"location":"current_concepts/#rag","title":"RAG","text":"<p>RAG = Retrieval Augmented Generation</p> <p>RETRIEVAL:</p> <p>We retrieve the most useful information from a vector database and then use it to generate a response using the LLM.</p> <p>AUGEMENTED:</p> <p>We combine the retreived documents and the query and pass it to the LLM to generate a response.</p> <p>GENERATION:</p> <p>The LLM generates the final response with the augmented context.</p>"},{"location":"current_concepts/#prompt_engineering","title":"Prompt Engineering","text":"<p>Now the preferred term is flow engineering.</p> <p>There is a difference between sending the documents and query to the LLM on their own compared to using a prompt template:</p> <p>====================================</p> <p>\"You are an expert in the field of medicine. Answer the following question: {question} based on the following documents: {documents}\"</p> <p>If you are unsure about your answer, say \"I don't know\".</p> <p>Use a professional and formal language in your responses.</p> <p>Where possible, use references from the documents.</p> <p>====================================</p> <p>I like to thing of it as giving an actor details about a scene or giving a detailed job description to an researcher.</p> <p>It is why many of the tech leaders say the new programming language is English/Natural Language. Our work is to create detailed and informative specs for the LLMs.</p>"},{"location":"current_concepts/#ai_agents","title":"AI Agents","text":"<p>What is an Agent? </p> <p>Anthropic says:</p> <p></p> <p>LLMs are well established in the field of AI Agents. We create a profile of an agent with instructions and then it can carry out that task, calling upon tools we may have given it.</p> <p>An example:</p> <p></p> <p>====================================</p> <p>You are an expert medical reviewer. Grade the following content for accuracy and completeness. If you are unable to do this then say \"I don't know\".</p> <p>If the article is acceptible, return \"Acceptable\", otherwise return \"Not Acceptable\".</p> <p>You have the following tools available:</p> <ul> <li>Check Accuracy Tool</li> <li>Check Completeness Tool</li> <li>Check Relevancy Tool</li> </ul> <p>Here is the content to be graded:</p> <p>[article]</p> <p>====================================</p> <p>Whilst we may have our overall flowchart, the AI Agent will determine the path to take based on the output the LLM generates.</p> <p>Langflow is a framework for building AI Agents and workflows. </p> <p>We can see from the snippet of code how LangGraph simplifies the process.    </p> <p></p> <p>In essence, we can convert our flowchart into code using Langflow.</p>"},{"location":"current_concepts/#naive_rag","title":"Naive Rag","text":"<p>Two years ago when we first looked at RAG, the strategy was to take a document and split into chuncks of say 500 characters and have an overlap of 100 charactes with the next chunk to maintain some context.</p> <p>This works very well for single modality text documents where this Niave Rag is in effect a search tool, taking an unstructured query and using LLMs and semantic search to find relevant chunks and answer the question.</p>"},{"location":"current_concepts/#changes_in_last_2yrs","title":"Changes in last 2yrs","text":"<p>In the past two years there have been many changes in the space.</p> <ul> <li>Context windows, the amount of text we can pass to the LLM has grown from 4K to 32K+</li> <li>Compute power has greatly increased, costs have gone donw by 90%.</li> <li>LLMs have become bigger and better, with many specialist LLMs that are fine tuned for particular uses.</li> <li>LLMs can now understand images, tables and videos better, with LLMs that can parse different media and convert them into text.</li> <li>Many frameworks and services for RAG have emerged and 2025 is thought of the year that the question will be which system to use?</li> <li>Naive Rag has been replaced with more sophisticated strategies that involve all aspects of the architecture:</li> </ul> <p>Without needing to understand this next image, here is the current 'state of the art' RAG architecture:</p> <p></p> <p>New academic papers for better techniques are continually being published.</p> <p>\"This is the worst it will ever be...\" - someone said.</p>"},{"location":"current_concepts/#proposed_application","title":"Proposed application","text":"<p>First and most importantly we will look at how we ingest articles into our DB.</p>"},{"location":"current_concepts/#unstructuredio","title":"unstructured.io","text":"<p>unstructured.io is a leader in parsing all types of files into strucutred data.</p> <p>If we think of our pdf as a molecule, then we can break it down into atoms containg the text, images and tables as well as metadata.</p> <p>This can then be entered into our DB with the apporpriate part vectorised for semantic search.</p> <pre><code>[\n    {\n        \"type\": \"Title\",\n        \"element_id\": \"0405351ac64213c7b1e40e31aff7d21b\",\n        \"metadata\": {\n            \"filename\": \"Bank Good Credit Loan.pptx\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:16:14\",\n            \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n            \"category_depth\": 1,\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 1\n        },\n        \"text\": \"Bank Good Credit \"\n    },\n    {\n        \"type\": \"NarrativeText\",\n        \"element_id\": \"214987ebee9fd615365185fb3d692253\",\n        \"metadata\": {\n            \"filename\": \"Bank Good Credit Loan.pptx\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:16:14\",\n            \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n            \"parent_id\": \"0405351ac64213c7b1e40e31aff7d21b\",\n            \"category_depth\": 0,\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 1\n        },\n        \"text\": \"Accredited with IABAC\u2122\"\n    },\n    {\n        \"type\": \"ListItem\",\n        \"element_id\": \"5eb6ec96e6a3493c1ae56747ae457b7f\",\n        \"metadata\": {\n            \"filename\": \"Bank Good Credit Loan.pptx\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:16:14\",\n            \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n            \"parent_id\": \"2dc308bd8d3a5c745dfacc3bdccd81db\",\n            \"category_depth\": 1,\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 2\n        },\n        \"text\": \"Data provided\"\n    },\n    {\n        \"type\": \"Table\",\n        \"element_id\": \"0f932c1c78cd59aef141af819dfdcf84\",\n        \"metadata\": {\n            \"filename\": \"currency.csv\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:17:41\",\n            \"filetype\": \"text/csv\",\n            \"languages\": [\n                \"eng\"\n            ],\n            \"text_as_html\": \"&lt;table border=\\\"1\\\" class=\\\"dataframe\\\"&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Code&lt;/td&gt;\\n      &lt;td&gt;Symbol&lt;/td&gt;\\n      &lt;td&gt;Name&lt;/td&gt;\\n    &lt;/tr&gt;\\n    td&gt;\\n      &lt;td&gt;Ft&lt;/td&gt;\\n           &lt;td&gt;ZK&lt;/td&gt;\\n      &lt;td&gt;Zambian kwacha&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\"\n        },\n        \"text\": \"\\n\\n\\nCode\\nSymbol\\nName\\n\\n\\nAED\\n\\u062f.\\u0625\\nUnited Arab Emirates d\\n\\n\\nAFN\\n\\u060b\\nAfghan afghani\\n\\n\\nALL\\nL\\nAlbanian lek\\n\\n\\nAMD\\nAMD\\nArmenian dram\\n\\n\\nANG\\n\\u0192\\nNetherlands Antillean gu\\n\\n\\nAOA\\nKz\\nAngolan kwanza\\n\\n\\nARS\\n$\\nArgentine i rial\\n\\n\\nZAR\\nR\\nSouth African rand\\n\\n\\nZMW\\nZK\\nZambian kwacha\\n\\n\\n\"\n    },\n    {\n        \"type\": \"Image\",\n        \"element_id\": \"37883f438c468b3027dd7918a958dacd\",\n        \"text\": \"15 - &amp; \\u2014 10g =} \\u2014 &amp; \\u20146g N Potential (V) \\u2014 2 &amp; = Control 2 e 88 25 T T 0.0000001 0.00001 0.001 01 Current Density (A/cm2)\",\n        \"metadata\": {\n            \"image\": \"base64 value\",\n            \"filetype\": \"application/pdf\",\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 1,\n            \"filename\": \"embedded-images-tables.pdf\",\n            \"data_source\": {\n                \"url\": null,\n                \"version\": null,\n                \"record_locator\": {\n                    \"path\": \"C:\\\\Users\\\\mrcra\\\\Desktop\\\\RAG\\\\pdf_input\\\\embedded-images-tables.pdf\"\n                },\n                \"date_created\": \"1727326976.0\",\n                \"date_modified\": \"1727326976.0\",\n                \"date_processed\": \"1729367235.6743865\",\n                \"permissions_data\": [\n                    {\n                        \"mode\": 33206\n                    }\n                ],\n                \"filesize_bytes\": 109798\n            }\n        }\n\n]\n</code></pre> <p>The cost for single-modality is $10 for 10,000 pages and for multi-modal it is \u00a310 for 1,000 pages.</p> <p>A no-brainer...</p> <p>Vision Language model using whole PDF and convert query into VLM vector is very effective.</p> <p>Unstructured.io has a bunch of LLMs that can process any type of data and probbably does this under the hood.</p> <ul> <li>Probablistic Search -User Feedback Part</li> <li>Vector Search Model - LLM </li> <li>Bindary serach - Traditional Postgres</li> </ul>"},{"location":"current_concepts/#text_processing","title":"Text processing","text":"<p>In the case of text, we can embed the content for semantic search.</p> <p>It also means we can create articles by gathering relative text fragments and , of course, we can add tables and images.</p>"},{"location":"current_concepts/#table_processing","title":"Table processing","text":"<p>We get both text_html and text_plain as output.</p>"},{"location":"current_concepts/#image_processing","title":"Image processing","text":"<p>In the case of an image, we get the Base64 value of the image. </p> <p>We don't need to reference a stored image but have it in the outputted element.</p> <p>This can then be processed with an LLM to get a text description of the image, in addition to the labels attached to the image.</p>"},{"location":"current_concepts/#audio","title":"Audio","text":"<p>Not part of unstructured.io but speech to text APIs are available and well established. We can process the text in the usual way.</p>"},{"location":"current_concepts/#video","title":"Video","text":"<p>Not part of unstructured.io but YouTube has an API to get transcripts of videos which can then be processed as we would process any text.</p> <p>Videos are a series of images so there are ways to create descriptions of videos etc.</p>"},{"location":"current_concepts/#workflow","title":"Workflow","text":""},{"location":"current_concepts/#flowchart","title":"FLOWCHART","text":"<p>Using Langflow, we can create a workflow to process the documents. In essence, we first create our own flowchart for human processing and then convert this to code.</p> <p>Langflow has NODES (entities) and EDGES (connections between nodes). They can be 1:1 or conditional (if/else).</p> <p></p>"},{"location":"current_concepts/#get_answer_from_cache","title":"GET ANSWER FROM CACHE","text":"<p>When a user asks a question, we will first look in the DB if there is already an answer to a similar question. This is built up from the USER RATE FILTER strategy and initially will be empty.</p>"},{"location":"current_concepts/#do_semantic_search","title":"DO SEMANTIC SEARCH","text":"<p>If there is no answer, we will then run the Langflow workflow to generate an answer.</p> <p>We can do REFINE_QUESTION as a technique to get better answers. This will involve a number of different strategies outline in the . <p>Having a refined question or set of questions, we will then retrieve the best K results.</p>"},{"location":"current_concepts/#rank_filter_docs","title":"RANK &amp; FILTER DOCS","text":"<p>We can then apply strategies to rank the documents and filter out irrelevant ones.</p>"},{"location":"current_concepts/#generate_response","title":"GENERATE RESPONSE","text":"<p>We send the context content and (refined) question(s) to the LLM to get the answer.</p>"},{"location":"current_concepts/#check_no_hallucinations","title":"CHECK NO HALLUCINATIONS","text":"<p>We will then do CHECK_NO_HALLUCINATIONS to check that the final answer is not hallucinated and then send the generated response to the user. We can use reflective RAG to see if we need to repeat the process.</p>"},{"location":"current_concepts/#user_rate_and_cache","title":"USER RATE AND CACHE","text":"<p>The user will then do USER_RATE where the user can give thumbs up/down or rate for accuracy, content, relevancy and clarity as well as adding their own comments and additions to the response.</p> <p>We will then cache (store) this in the DB for future use.</p> <p>Obviously, we can use a number of different strategies to generate the final answer.</p>"},{"location":"current_concepts/#graph_rag","title":"GRAPH RAG","text":"<p>One emerging technology is GRAPHRAG. Here, the standard database we use for keyword and semantic search is converted to a Graph Database so that NODES (entities) and EDGES (connections between nodes) can be formed not just within each article but between articles.</p> <p>What if we could run a query that gathers all connected and relevant atoms to create a super-article? (Instead of creating this 'manually' through imperative code).</p> <p></p>"},{"location":"more/","title":"AI Powered Knowledge Systems","text":"<p>AI Powered Knowledge Systems is an architecture and app to enable organisations to search through their knowledge base for answers to questions and use AI Agents to create reports and other content.</p> <p>There are a number of core components in the architecture and these are listed below.</p>"},{"location":"more/#database","title":"Database","text":"<p>We use Postgresql as the database and rather than process documents with each query, we use databases to store the documents and then use keyword and semantic search to find the most relevant elements of information. To enhance speed and accuracy, further tradional Natural Language Processing is applied.</p> <p>I have found https://unstructured.io/ to be a leader in parsing all types of files into structured data. The cost for multi-modal files is $10 for 1,000 pages and for single-modal it is $10 for 10,000 pages.</p> <p>This is a no brainer cost wise as far as I am concerned.</p> <p></p> <ul> <li>We extract all the 'atoms' of information from 'molecules' of articles, like paragraphs, tables, images etc.</li> <li>Using unstructured.io we can parse any type of file into structured data.</li> <li>Any tabular or image atoms can be further processed using LLMs to generate content and metadata.</li> <li>The atoms are then stored in a database. Vectorisation can take place in the database when we use PostgresML or done separately.</li> <li>We can create Named Entities and use them as tags.</li> <li>We then use LLMs and semantic search along with keyword search to answer questions form a user.</li> <li>An emerging technology is to use Graph Databases to develop retlationships between atoms and molecules for further retrieval capabilites.</li> </ul> <p>The atoms are stored in a database in the following format with additonal Natural Language Processing techniques applied to speed up future search:</p> <p></p>"},{"location":"more/#retrieval_strategies","title":"Retrieval Strategies","text":"<p>There are many and growing number of strategies for doing semantic search.</p> <p>The image below is for illustrative rather than informative puproses and shows the growing number of strategies that are being implemented. </p> <p>These would be modular and different ones can be used based on user queries.</p> <p></p>"},{"location":"more/#crowd_sourced_answers","title":"Crowd Sourced Answers","text":""},{"location":"more/#ai_agents","title":"AI Agents","text":"<p>The definition of and Agent is quite broad and varied.</p> <p>Github states: \"Agentic AI refers to artificial intelligence capable of making decisions, planning, and adapting to new information in real time. AI agents learn and enhance their performance through feedback, utilizing advanced algorithms and sensory inputs to execute tasks and engage with their environments.\"</p> <p>Perhaps a simple version is from Langchain, which defines and agent as using LLM to determin the flow of an app.</p> <p>Using Langgraph, we can convert business case flowcharts to Agentic workflows.</p> <p>This is the compiled workflow for an example Langgraph workflow.</p> <p>All nodes and edges are customisable and we can add/delete nodes and edges as required.</p> <p></p> <p>The workflow we will using is the following:</p> <p></p> <ul> <li>Better questions get better answers so we get the LLM to write 5 versions of the user's question and answer them all.</li> <li>We rerank the documents based on the quality of the answers.</li> <li>We filter out documents that are not relevant to the question.</li> <li>We can then check that the answer is not hallucinated.</li> <li>With the content and sources, we can generate a report or answer the user's question.</li> <li>We can also connect to other data sources to enhance the answers and reports.</li> </ul>"},{"location":"more/#database_queries","title":"Database Queries","text":"<p>In our database we can then reconstuct any article through various strategies of gathering and joining atoms.</p> <p>We have 4 ways of performing queries</p> <ol> <li> <p>Traditional binary search of SQL. This is incredibly powerful for filtering out data not relevant to the query, like search by year, topic, author etc. We can tokenize and perform Named Entity Recognition with MedSpacy to create tags for each atom.</p> </li> <li> <p>Vector search based on semantic similarity using LLMs.</p> </li> <li> <p>Graph based serach using relationships between entities. This is an emerging technology for RAG, (Retreival Augmented Generation which will be explained later). One example would be a query that asks for information for a collection of authors who have worked together  previously but not currently. Or we may detect entities in the text that are not encapsulated in the database but would be useful in queries.</p> </li> <li> <p>Crowd sourced answers - when a user runs a query and gets a result, we can ask the user to rate it and add their own comments. There are a number of features to this which will be discussed later. This will also provide a dataset for further ML options.</p> </li> </ol> <p>With this atomic version of data, we can carry out summarisation, queries etc. and also build reports that aggregate information from many sources.</p>"},{"location":"more/#costs_and_income","title":"Costs and Income","text":""},{"location":"more/#data_ingestion","title":"Data Ingestion","text":"<p>We have seen previously that unstructured.io costs $10 for 1,000 muliti-modal pages and $10 for 10,000 single-modal pages. If we further process tabular data and images there are LLM costs for this.</p> <p>Using a wighted average of the costs for 1,000 and 10,000 pages we get a cost of $5 for 1,000 pages and $50 for 10,000 pages.</p>"},{"location":"more/#servers","title":"Servers","text":"<p>Hosting a database and API has a base cost of $30-100 per month dependin gon type of infrastructure used. This can then scale up as required, which will incur additionsal costs but these would be covered by users.</p> <p>LLM costs can be covered by the user entering their own API key to use OpenAI for example or the cost could be calculated based on useage and billed accordingly.</p> <p>Income can be generated on a Pay As you Go basis or a subscription model. A hybrid is also possible.</p>"},{"location":"more/#monitoring","title":"Monitoring","text":"<p>If this is a Sofware as a Service, (SASS), product then it is important to monitor the performance of the system and handle user queries.</p> <p>Like any business, this would be based on financial principles.</p>"},{"location":"more/#development","title":"Development","text":"<p>Coding is 10% of the project.</p> <p>Data collection and preparation, combined with monitoring the systems is the other 90%.</p> <p>Evaluating performance of the system is also important.</p> <p>What is important to remember is that this system is not just a Knowledge Base but also an AI Research Assistant.</p>"},{"location":"proposal/","title":"AI Powered Knowledge Systems","text":""},{"location":"proposal/#mission","title":"Mission","text":"<p>To create a Software as a Service, (SASS), product that enable the medical community to harness the power of AI to improve extraction and presentation of relevent, accurate and complete information from the vast amount of data available in the medical literature.</p>"},{"location":"proposal/#key_points","title":"Key points","text":""},{"location":"proposal/#how","title":"How","text":"<p>We break up the molecules of an article into atoms, getting their content and metadata, and then use LLMs and semantic search to answer questions form a user.</p> <p>If the atoms contain tabular data, we will have both the text and HTML content so we can use an LLM to get information about it.</p> <p>If it has an image, we can use an LLM to get information about it as well.</p> <p>It is multi-modal covering a great range of file types like Word, Powerpoint and we can also process audio and video.</p> <p>In our database we can then reconstuct any article through various strategies of gathering and joining atoms.</p> <p>We have 4 ways of performing queries</p> <ol> <li> <p>Traditional binary search of SQL. This is incredibly powerful for filtering out data not relevant to the query, like search by year, topic, author etc. We can tokenize and perform Named Entity Recognition with MedSpacy to create tags for each atom.</p> </li> <li> <p>Vector search based on semantic similarity using LLMs.</p> </li> <li> <p>Graph based serach using relationships between entities. This is an emerging technology for RAG, (Retreival Augmented Generation which will be explained later).</p> </li> <li> <p>Crowd sourced answers - when a user runs a query and gets a result, we can ask the user to rate it and add their own comments. There are a number of features to this which will be discussed later. This will also provide a dataset for further ML options.</p> </li> </ol> <p>With this atomic version of data, we can carry out summarisation, queries etc. and also build reports that aggregate information from many sources.</p>"},{"location":"proposal/#current_concepts","title":"Current concepts","text":"<p>Let's take a look at some of the current buzz words in this space so that we understand what we are doing.</p>"},{"location":"proposal/#llm","title":"LLM","text":"<p>A large language model (LLM) is a type of artificial intelligence (AI) program that can recognize and generate text, among other tasks. LLMs are trained on huge sets of data \u2014 hence the name \"large.\" LLMs are built on machine learning: specifically, a type of neural network called a transformer model.</p> <p>We represent it in our architecture as a brain and this serves us well as we can also think what would a human do at this stage?</p> <p></p>"},{"location":"proposal/#semantic_search","title":"Semantic search","text":"<p>Semantic search vectors are numerical representations of data and related contexts that are used to rank and deliver search results based on their relevance. </p> <p>The closer two points in vector space are, the more similar they are.</p> <p></p> <p>OpenAI has 1536 dimensions...</p> <p>There are a few different methods such as cosine similarity, dot product, Euclidean distance etc.</p>"},{"location":"proposal/#rag","title":"RAG","text":"<p>RAG = Retrieval Augmented Generation</p> <p>RETRIEVAL:</p> <p>We retrieve the most useful information from a vector database and then use it to generate a response using the LLM.</p> <p>AUGEMENTED:</p> <p>We combine the retreived documents and the query and pass it to the LLM to generate a response.</p> <p>GENERATION:</p> <p>The LLM generates the final response.</p>"},{"location":"proposal/#prompt_engineering","title":"Prompt Engineering","text":"<p>Now the preferred term is flow engineering.</p> <p>There is a difference between sending the documents and query to the LLM on their own compared to using a prompt template:</p> <p>==================================================================</p> <p>\"You are an expert in the field of medicine. Answer the following question: {question} based on the following documents: {documents}\"</p> <p>If you are unsure about your answer, say \"I don't know\".</p> <p>Use a professional and formal language in your responses.</p> <p>Where possible, use references from the documents.</p> <p>==================================================================</p> <p>I like to thing of it as giving an actor details about a scene or giving a detailed job description to an researcher.</p>"},{"location":"proposal/#ai_agents","title":"AI Agents","text":"<p>LLMs are well established in the field of AI Agents. We create a profile of an agent with instructions and then it can carry out that task, calling upon tools we may have given it.</p> <p>An example:</p> <p>=============================================================</p> <p>You are an expert medical reviewer. Grade the following content for accuracy and completeness. If you are unable to do this then say \"I don't know\".</p> <p>If the article is acceptible, return \"Acceptable\", otherwise return \"Not Acceptable\".</p> <p>You have the following tools available:</p> <ul> <li>Check Accuracy Tool</li> <li>Check Completeness Tool</li> <li>Check Relevancy Tool</li> </ul> <p>Here is the content to be graded:</p> <p>[article]</p> <p>=============================================================</p> <p>Whilst we may have our overall flowchart, the AI Agent will determine the path to take based on the output the LLM generates.</p> <p>Langflow is a framework for building AI Agents and workflows. </p> <p>We can see from the snippet of code how LangGraph simplifies the process.    </p> <p></p> <p>In essence, we can convert our flowchart into code using Langflow.</p>"},{"location":"proposal/#naive_rag","title":"Naive Rag","text":"<p>Two years ago when we first looked at RAG, the strategy was to take a document and split into chuncks of say 500 characters and have an overlap of 100 charactes with the next chunk to maintain some context.</p> <p>This works very well for single modality text documents where this Niave Rag is in effect a search tool, taking an unstructured query and using LLMs and semantic search to find relevant chunks and answer the question.</p>"},{"location":"proposal/#changes_in_2yrs","title":"Changes in 2yrs","text":"<p>In the past two years there have been many changes in the space.</p> <ul> <li>Context windows, the amount of text we can pass to the LLM has grown from 4K to 32K+</li> <li>Compute power has greatly increased, costs have gone donw by 90%.</li> <li>LLMs have become bigger and better, with many specialist LLMs that are fine tuned for particular uses.</li> <li>LLMs can now understand images, tables and videos better, with LLMs that can parse different media and convert them into text.</li> <li>Many frameworks and services for RAG have emerged and 2025 is thought of the year that the question will be which system to use?</li> <li>Naive Rag has been replaced with more sophisticated strategies that involve all aspects of the architecture:</li> </ul> <p>Without needing to understand this next image, here is the current 'state of the art' RAG architecture:</p> <p></p> <p>New academic papers for better techniques are continually being published.</p> <p>\"This is the worst it will ever be...\" - someone said.</p>"},{"location":"proposal/#proposed_application","title":"Proposed application","text":"<p>First and most importantly we will look at how we ingest articles into our DB.</p>"},{"location":"proposal/#unstructuredio","title":"unstructured.io","text":"<p>unstructured.io is a leader in parsing all types of files into strucutred data.</p> <p>If we think of our pdf as a molecule, then we can break it down into atoms containg the text, images and tables as well as metadata.</p> <p>This can then be entered into our DB with the apporpriate part vectorised for semantic search.</p> <pre><code>[\n    {\n        \"type\": \"Title\",\n        \"element_id\": \"0405351ac64213c7b1e40e31aff7d21b\",\n        \"metadata\": {\n            \"filename\": \"Bank Good Credit Loan.pptx\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:16:14\",\n            \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n            \"category_depth\": 1,\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 1\n        },\n        \"text\": \"Bank Good Credit \"\n    },\n    {\n        \"type\": \"NarrativeText\",\n        \"element_id\": \"214987ebee9fd615365185fb3d692253\",\n        \"metadata\": {\n            \"filename\": \"Bank Good Credit Loan.pptx\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:16:14\",\n            \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n            \"parent_id\": \"0405351ac64213c7b1e40e31aff7d21b\",\n            \"category_depth\": 0,\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 1\n        },\n        \"text\": \"Accredited with IABAC\u2122\"\n    },\n    {\n        \"type\": \"ListItem\",\n        \"element_id\": \"5eb6ec96e6a3493c1ae56747ae457b7f\",\n        \"metadata\": {\n            \"filename\": \"Bank Good Credit Loan.pptx\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:16:14\",\n            \"filetype\": \"application/vnd.openxmlformats-officedocument.presentationml.presentation\",\n            \"parent_id\": \"2dc308bd8d3a5c745dfacc3bdccd81db\",\n            \"category_depth\": 1,\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 2\n        },\n        \"text\": \"Data provided\"\n    },\n    {\n        \"type\": \"Table\",\n        \"element_id\": \"0f932c1c78cd59aef141af819dfdcf84\",\n        \"metadata\": {\n            \"filename\": \"currency.csv\",\n            \"file_directory\": \"tmpdocs\",\n            \"last_modified\": \"2023-11-02T15:17:41\",\n            \"filetype\": \"text/csv\",\n            \"languages\": [\n                \"eng\"\n            ],\n            \"text_as_html\": \"&lt;table border=\\\"1\\\" class=\\\"dataframe\\\"&gt;\\n  &lt;tbody&gt;\\n    &lt;tr&gt;\\n      &lt;td&gt;Code&lt;/td&gt;\\n      &lt;td&gt;Symbol&lt;/td&gt;\\n      &lt;td&gt;Name&lt;/td&gt;\\n    &lt;/tr&gt;\\n    td&gt;\\n      &lt;td&gt;Ft&lt;/td&gt;\\n           &lt;td&gt;ZK&lt;/td&gt;\\n      &lt;td&gt;Zambian kwacha&lt;/td&gt;\\n    &lt;/tr&gt;\\n  &lt;/tbody&gt;\\n&lt;/table&gt;\"\n        },\n        \"text\": \"\\n\\n\\nCode\\nSymbol\\nName\\n\\n\\nAED\\n\\u062f.\\u0625\\nUnited Arab Emirates d\\n\\n\\nAFN\\n\\u060b\\nAfghan afghani\\n\\n\\nALL\\nL\\nAlbanian lek\\n\\n\\nAMD\\nAMD\\nArmenian dram\\n\\n\\nANG\\n\\u0192\\nNetherlands Antillean gu\\n\\n\\nAOA\\nKz\\nAngolan kwanza\\n\\n\\nARS\\n$\\nArgentine i rial\\n\\n\\nZAR\\nR\\nSouth African rand\\n\\n\\nZMW\\nZK\\nZambian kwacha\\n\\n\\n\"\n    },\n    {\n        \"type\": \"Image\",\n        \"element_id\": \"37883f438c468b3027dd7918a958dacd\",\n        \"text\": \"15 - &amp; \\u2014 10g =} \\u2014 &amp; \\u20146g N Potential (V) \\u2014 2 &amp; = Control 2 e 88 25 T T 0.0000001 0.00001 0.001 01 Current Density (A/cm2)\",\n        \"metadata\": {\n            \"image\": \"base64 value\",\n            \"filetype\": \"application/pdf\",\n            \"languages\": [\n                \"eng\"\n            ],\n            \"page_number\": 1,\n            \"filename\": \"embedded-images-tables.pdf\",\n            \"data_source\": {\n                \"url\": null,\n                \"version\": null,\n                \"record_locator\": {\n                    \"path\": \"C:\\\\Users\\\\mrcra\\\\Desktop\\\\RAG\\\\pdf_input\\\\embedded-images-tables.pdf\"\n                },\n                \"date_created\": \"1727326976.0\",\n                \"date_modified\": \"1727326976.0\",\n                \"date_processed\": \"1729367235.6743865\",\n                \"permissions_data\": [\n                    {\n                        \"mode\": 33206\n                    }\n                ],\n                \"filesize_bytes\": 109798\n            }\n        }\n\n]\n</code></pre> <p>The cost for single-modality is $10 for 10,000 pages and for multi-modal it is \u00a310 for 1,000 pages.</p> <p>A no-brainer...</p> <p>Vision Language model using whole PDF and convert query into VLM vector is very effective.</p> <p>Unstructured.io has a bunch of LLMs that can process any type of data and probbably does this under the hood.</p> <ul> <li>Probablistic Search -User Feedback Part</li> <li>Vector Search Model - LLM </li> <li>Bindary serach - Traditional Postgres</li> </ul>"},{"location":"proposal/#text_processing","title":"Text processing","text":"<p>In the case of text, we can embed the content for semantic search.</p> <p>It also means we can create articles by gathering relative text fragments and , of course, we can add tables and images.</p>"},{"location":"proposal/#table_processing","title":"Table processing","text":"<p>We get both text_html and text_plain as output.</p>"},{"location":"proposal/#image_processing","title":"Image processing","text":"<p>In the case of an image, we get the Base64 value of the image. </p> <p>We don't need to reference a stored image but have it in the outputted element.</p> <p>This can then be processed with an LLM to get a text description of the image, in addition to the labels attached to the image.</p>"},{"location":"proposal/#audio","title":"Audio","text":"<p>Not part of unstructured.io but speech to text APIs are available and well established. We can process the text in the usual way.</p>"},{"location":"proposal/#video","title":"Video","text":"<p>Not part of unstructured.io but YouTube has an API to get transcripts of videos which can then be processed as we would process any text.</p> <p>Videos are a series of images so there are ways to create descriptions of videos etc.</p>"},{"location":"proposal/#workflow","title":"Workflow","text":""},{"location":"proposal/#flowchart","title":"FLOWCHART","text":"<p>Using Langflow, we can create a workflow to process the documents. In essence, we first create our own flowchart for human processing and then convert this to code.</p> <p>Langflow has NODES (entities) and EDGES (connections between nodes). They can be 1:1 or conditional (if/else).</p> <p></p> <p></p>"},{"location":"proposal/#get_answer_from_cache","title":"GET ANSWER FROM CACHE","text":"<p>When a user asks a question, we will first look in the DB if there is already an answer to a similar question. This is built up from the USER RATE FILTER strategy and initially will be empty.</p>"},{"location":"proposal/#do_semantic_search","title":"DO SEMANTIC SEARCH","text":"<p>If there is no answer, we will then run the Langflow workflow to generate an answer.</p> <p>We can do REFINE_QUESTION as a technique to get better answers. This will involve a number of different strategies outline in the . <p>Having a refined question or set of questions, we will then retrieve the best K results.</p>"},{"location":"proposal/#rank_filter_docs","title":"RANK &amp; FILTER DOCS","text":"<p>We can then apply strategies to rank the documents and filter out irrelevant ones.</p>"},{"location":"proposal/#generate_response","title":"GENERATE RESPONSE","text":"<p>We send the context content and (refined) question(s) to the LLM to get the answer.</p>"},{"location":"proposal/#check_no_hallucinations","title":"CHECK NO HALLUCINATIONS","text":"<p>We will then do CHECK_NO_HALLUCINATIONS to check that the final answer is not hallucinated and then send the generated response to the user. We can use reflective RAG to see if we need to repeat the process.</p>"},{"location":"proposal/#user_rate_and_cache","title":"USER RATE AND CACHE","text":"<p>The user will then do USER_RATE where the user can give thumbs up/down or rate for accuracy, content, relevancy and clarity as well as adding their own comments and additions to the response.</p> <p>We will then cache (store) this in the DB for future use.</p> <p>Obviously, we can use a number of different strategies to generate the final answer.</p>"},{"location":"proposal/#graph_rag","title":"GRAPH RAG","text":"<p>One emerging technology is GRAPHRAG. Here, the standard database we use for keyword and semantic search is converted to a Graph Database so that NODES (entities) and EDGES (connections between nodes) can be formed not just within each article but between articles.</p> <p>What if we could run a query that gathers all connected and relevant atoms to create a super-article? (Instead of creating this 'manually' through imperative code).</p>"},{"location":"proposal/#requirements","title":"Requirements","text":""},{"location":"proposal/#data","title":"Data","text":"<p>We will need a set of pdfs, Word, Powerpoint and Excel documents etc for a particular domain area to carry out evaluation of the app.</p>"},{"location":"proposal/#what_to_do","title":"What to do?","text":"<p>Determine what users will want to do, like summarising, collating, queries etc.</p> <p>What would a human researcher want and what would they do to get their final response?</p>"},{"location":"proposal/#benchmarking","title":"Benchmarking","text":"<p>A number of benchmarking sets of questions that can be processed to get generated responses. These will be in a spreadsheet with metrics and overall rating for a domain expert to evaluate the app.</p> <p></p> <p>THIS IS THE ULTIMATE EVALUATION OF THE APP</p>"},{"location":"proposal/#hosting","title":"Hosting","text":"<p>This is detailed in the architecture section.</p> <p>Postgres/Django on render.com.</p> <p>The front end is decoupled from backend so that we can change the technology of the front end as we see fit. It is the embellishment part but having a range of functionality for the user is important as they must be able to select whaht they want to do with options.</p>"},{"location":"proposal/#timescale","title":"Timescale","text":"<p>Most of the core is done soe March-June will be final develpoment, testing and evaluation. </p> <p>By the end of June, a SaSSO will be ready to go live or the project will come to an end.</p>"},{"location":"proposal/#costs","title":"Costs","text":"<p>The cost for unstructured.io for say 50 articles = 500 pages is a nominal $5.</p> <p>When useage grows, the costs will increase. These will need to be borne by the project or users.</p> <p>For customers, they can use their own OpenAI account. Charges for DB and website traffic will be borne by the project/customer/ </p> <p>This mechanism is yet to be determined.</p> <p></p>"},{"location":"talks/","title":"Talks and Workshops","text":""},{"location":"talks/#python","title":"Python","text":"<ul> <li> <p>DjangoConEurope April 2025: Implementing Agentic AI solutions in Django from scratch - 90 minute workshop.</p> </li> <li> <p>Brighton Py Feb 2025: AI as API in everyday Python apps - 60 minute talk and demo.</p> </li> <li> <p>Django Japan Congress Feb 2025 (online): Implementing Agentic AI solutions in Django from scrathc - 45 minute talk</p> </li> <li> <p>Conf42 Feb 2025  (online): Implementing Agentic AI Solutions in Python from scratch - 50 minute talk.</p> </li> <li> <p>PyCon Ireland Nov 2024: Getting started with Pytest - 2 hr workshop.</p> </li> </ul> <p>Next...</p> <ul> <li>AI Agents in the Data Pipeline - 90 minute workshop. Awaiting to be announced.</li> </ul>"},{"location":"talks/#other","title":"Other","text":"<ul> <li> <p>TALK: Offline and instant websites, aka Progressive Web Apps - AsyncJS, Brighton, September 2021.</p> </li> <li> <p>LIGHTNING TALK: WordPress as a Micro Service to any framework - WordFest, July 2021.</p> </li> <li> <p>TALK: WP REST API and Web Components =&gt; 100% Internet - WordCamp Santa Clarita, July 2021.</p> </li> <li> <p>TALK: Web Components in WP, Gutenberg and as HTML plugins. - WordCamp North East Ohio May 2021.</p> </li> <li> <p>TALK: Leveraging the power or the WordPress REST API - WP Leeds April 2021</p> </li> <li> <p>WORKSHOP: WP REST API and you -&gt; Best Friends Forever workshop (90 mins) - WordCamp Greece April 2021. Code: https://github.com/iwswordpress/WordCampGreece</p> </li> <li> <p>TALK: Web Components as Micro Apps - NDC London, Jan 2021</p> </li> <li> <p>TALK: Unifying frameworks with Web Components - Brighton AsyncJS, Nov 2020.</p> </li> <li> <p>WORKSHOP: Progressive Web Apps Workshop (2hrs) - NDC Oslo June 2020 and a paid training workshop with NDC.</p> </li> <li> <p>WORKSHOP: Web Components Workshop (2hrs) - NDC Oslo June 2020 and a paid training workshope with NDC.</p> </li> <li> <p>WORKSHOP: Progressive Web Apps Workshop (2hrs) - Brighton WordUp June 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API with AJAX Forms and Pages - WordCamp Denver, June 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API with AJAX Forms and Pages - WordCamp Kent, Ohio May 2020.</p> </li> <li> <p>TALK: What is the WP REST API and how can I use it to make forms and pages that don\u2019t need to do be reloaded? - WordUp Brighton May 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API and AJAX Forms - WordCamp Geneva March 2020 [EVENT CANCELLED due to virus concerns :( </p> </li> <li> <p>TALK - WP-HTML: The marriage of WP and JS Frameworks for expansion, ubiquity and profit - WordCamp Vienna February 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API - WordCamp Vienna February 2020.</p> </li> <li> <p>TALK: Progressive Web Apps - Brighton WordUp November 2019.</p> </li> <li> <p>TALK: Decoupled WordPress (code along style) - WordCamp Dublin October 2019.</p> </li> <li> <p>TALK: JWT and Authentication - WPHooked London September 2019.</p> </li> <li> <p>TALK: Decoupled WordPress and WP Components - WordCamp Brighton August 2019.</p> </li> </ul>"},{"location":"tldr/","title":"RAG TLDR","text":""},{"location":"tldr/#in_essence","title":"In essence","text":""},{"location":"tldr/#bullet_points","title":"Bullet points","text":"<p>We split PDF molecules and other data sources into atoms and store them in a database. </p> <p>We further process these atoms using traditonal Natural Language Processing techniques as needed to enhance search and reporting. </p> <p>We vectorise these chunks to enable AI Semantic Search. </p> <p>We can use other LLMs to analyse tables and images and generate content and metadata. </p> <p>We use AI Agents to convert workflows into Agentic Retrieval Augmented Generation for Q &amp; A and report making. </p> <p>Users rate answers to provide a 'crowd sourced' information store - IS_ACCURATE, IS_RELEVANT, IS_COMPLETE and comments/notes added. We can use this information to provide fast answers or aggregate information from many sources along with the crowd source answers </p> <p>Importing and exporting of Knowledge Bases and other data sources enables pooling of knowledge between organisations. </p> <p>All componets are modular enabling customisation as needed as well as offering a range of options for users to choose from. </p> <p>What would a human researcher do...?</p>"},{"location":"tldr/#a_modular_system","title":"A modular system","text":""},{"location":"tldr/#details","title":"Details","text":""},{"location":"tldr/#processing_data_sources","title":"Processing data sources","text":"<p>If TEXT, we can use Natural Language Processing techniques to summarise, extract entities and rephrase content for better semantic search and also tradional Full Text Search in Postgressql database.</p> <p>If TABLE data, we get both the text summary and HTML table structure. We can then use other LLMs to query the data.</p> <p>If IMAGE, we store the image and summary and then use other LLMs to anyalyse the image.</p> <p>(Audio and Video can be processed to.) </p>"},{"location":"tldr/#process_information","title":"Process information","text":"<p>We can use Natural Language Processing techniques to summarise, extract entities and rephrase content for better semantic search and also tradional Full Text Search in Postgressql database.</p> <p>We can also use Graph Databases to develop retlationships between atoms and molecules for further retrieval capabilites once we know what relationships we are interested in.</p> <p>This provides an OpenVerdict type service for Question/Answers with sources.</p> <p></p>"},{"location":"tldr/#agentic_rag_strategies","title":"Agentic RAG strategies","text":"<p>There are many and growing number of strategies for doing semantic search. </p> <p>We can use AI Agents to be researchers creating report etc using the knowledge base or other external knowledge sources.</p> <p></p> <p>A protocol for Knowledge Base import/export with other knowledge sources enables pooling of knowledge. It also enable researchers 'notes' to be use, annoted with current verification status.</p> <p>Reports would list and summarise knowledge from:</p> <ul> <li>Crowd sourced answers</li> <li>Documents in the knowledge base</li> <li>External knowledge sources</li> <li>Custom workflows can be made using frameworks like Langgraph etc.</li> </ul> <p>Being a modular system, the system can be adapted as needed as well as offering a range of options for users to choose from.</p>"},{"location":"tldr/#user_rated_answers","title":"User rated answers","text":"<p>We also enable users to rate the responses creating a 'crowd sourced information store' that can either provide fast answers or aggregate information from many sources along with the crowd source answers.</p> <p></p> <p></p>"},{"location":"architecture/stack/","title":"Stack","text":""},{"location":"architecture/stack/#database","title":"Database","text":"<p>We will use PostgreML as our database.</p>"},{"location":"architecture/stack/#front_end","title":"Front End","text":"<p>The system is designed to be uncoupled to the application enabling the technology used to be readily changed.</p> <p>I am opting for a Django front end as this is a leading Python Content Management System and I am already familiar with the framework.</p> <p>For client-side reactivity, HTMX and Alpine.js are used. These are more than sufficinet to recreate the React/Vue experience.</p>"},{"location":"architecture/stack/#hosting","title":"Hosting","text":"<p>Ulimately, Microsoft Azure will be used to host the application and the database. It also has many Cognitive Search tools that we might include.</p> <p>For now, we will use <code>render.com</code> to host the application. It is an industrial level hosting platform.</p> <p>The database can be readily migrated to Azure.</p>"},{"location":"concepts/agents/","title":"Agents","text":"<p>An AI Agent is some code that has a conversation with an LLM to determine application flow.</p> <p>Agents have access to tools that enable them to carry out functions and return observations to the application layer along with NEXT steps.</p>"},{"location":"concepts/agents/#180_degrees","title":"180 degrees","text":"<p>There are two concepts that seem to turn coding upside down. Like turning a computer mouse 180 degrees, there is no more complexity involved, still just up, down, left and right but done in the opposite way to usual.</p>"},{"location":"concepts/agents/#autonomy","title":"Autonomy","text":"<p>Autonomous AI Agents are able to determnine a next step based on the LLM which can then be used by the application layer to move on to the next step. It can seem tricky to see if we are coding imperatively or whether we have turned over direction to the AI Agent.</p>"},{"location":"concepts/agents/#creating_an_endpoint","title":"Creating an endpoint","text":"<p>Prompt or Flow Engineering is where we add additional instruction to the query for the LLM to follow.</p>"},{"location":"concepts/agents/#natural_language","title":"Natural Language","text":"<p>In many ways it is like pseudo-code where we spec out what we want the endpoint API to do along with the data we send:</p> <pre><code>REACT_SYSTEM_PROMPT = \"\"\"\nYou operate by running a loop with the following steps: Thought, Action, Observation.\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags.\nYou may call one or more functions to assist with the user query. Don' make assumptions about what values to plug\ninto functions. Pay special attention to the properties 'types'. You should use those types as in a Python dict.\n\nFor each function call return a json object with function name and arguments within &lt;tool_call&gt;&lt;/tool_call&gt; XML tags as follows:\n\n&lt;tool_call&gt;\n{\"name\": &lt;function-name&gt;,\"arguments\": &lt;args-dict&gt;, \"id\": &lt;monotonically-increasing-id&gt;}\n&lt;/tool_call&gt;\n\nHere are the available tools / actions:\n\n&lt;tools&gt;\n%s\n&lt;/tools&gt;\n\nExample session:\n\n&lt;question&gt;What's the current temperature in Madrid?&lt;/question&gt;\n&lt;thought&gt;I need to get the current weather in Madrid&lt;/thought&gt;\n&lt;tool_call&gt;{\"name\": \"get_current_weather\",\"arguments\": {\"location\": \"Madrid\", \"unit\": \"celsius\"}, \"id\": 0}&lt;/tool_call&gt;\n\nYou will be called again with this:\n\n&lt;observation&gt;{0: {\"temperature\": 25, \"unit\": \"celsius\"}}&lt;/observation&gt;\n\nYou then output:\n\n&lt;response&gt;The current temperature in Madrid is 25 degrees Celsius&lt;/response&gt;\n\nAdditional constraints:\n\n- If the user asks you something unrelated to any of the tools above, answer freely enclosing your answer with &lt;response&gt;&lt;/response&gt; tags.\n\"\"\"\n</code></pre> <p>We can pass in data for the query to the LLM as a prompt template.</p> <p>These two concepts, Autonomy and Prompt Engineering, are counter intuitive based on how we have been coding.</p>"},{"location":"concepts/agents/#tools","title":"Tools","text":"<p>AI Agents can also have tools, functions, bound to them so that the LLM can select the most appropriate tool based on the response.</p> <p>The function and arguments are then executed and added back into the prompt.</p> <p>A canonical example is doing a web search for additional data. If we need the weather for a location as part of the query then we need to get the weather first. A tool that gets the weather from a Weather API can then be used.</p> <p>The code is run on our box not the LLMs.</p> <p>OpenAI shows this in the following image:</p> <p></p>"},{"location":"concepts/agents/#patterns","title":"Patterns","text":"<p>There are many patterns but 4 main ones are;</p> <ol> <li> <p>Reflection</p> </li> <li> <p>Tool Calling</p> </li> <li> <p>Planning</p> </li> <li> <p>Multi Agent</p> </li> </ol> <p>The last one, Multi Agent, is a pattern where Agents can connect with each other and various hierarchies can be built.</p> <p>Having a Supervisor Agent that manages agents and crews can be useful for large scale projects.</p> <p>It is worth noting that Agents do not have to have an LLM and can be coded in the traditonal way. These softwared designs are well established over decades.</p>"},{"location":"concepts/agents/#pydantic_ai","title":"Pydantic AI","text":"<p>...</p>"},{"location":"concepts/agents/#smolagents","title":"SMOLAGENTS","text":"<p>A new technology that combines LLM with Python rather than LLM and JSON for tool use...</p>"},{"location":"concepts/agents/#frameworks","title":"Frameworks","text":""},{"location":"concepts/fine_tuning/","title":"Fine tuning","text":"<p>See Openai Fine Tuning.</p> <p>RAG, Retrieval Augnented Generation, can be used to supplement the query with contextual data collected from our data sources. This resolves the issue that the LLM will not have been trained on our data.</p> <p>What can be done is to fine tune the LLM with our own dataset so that the LLM is trained to understand our data.</p> <p>Costs and time are important factors to consider, yet for many organisations, having a specialist LLM for their data is advantageous.</p> <p></p>"},{"location":"concepts/graphs/","title":"Graphs","text":""},{"location":"concepts/graphs/#graph_networks","title":"Graph Networks","text":""},{"location":"concepts/graphs/#graph_databases","title":"Graph Databases","text":"<p>A graph database is a database that stores data in the form of a graph.</p> <p>A graph is a collection of nodes and edges.</p> <p>A node is a vertex in the graph.</p> <p>An edge is a connection between two nodes.</p> <p>A graph database is a database that stores data in the form of a graph.</p> <p>Practical GraphRAG Making LLMs smarter with Knowledge Graphs \u2014 Alison Cossette (PyBay 2024):</p> <p>https://www.youtube.com/watch?v=duMF1GkXO-ohttps://www.youtube.com/watch?v=duMF1GkXO-o</p>"},{"location":"concepts/vector_databases/","title":"Vector Databases","text":""},{"location":"concepts/vector_databases/#sematic_search","title":"Sematic search","text":"<p>Unlike keyword search, semantic search uses the context of the document to rank and deliver search results based on their relevance.</p> <p>The closer two points in vector space are, the more similar they are.</p> <p>Cat, Feline, Kitten will be closer together that Dog, Canine, Pup.</p> <p>The image above shows a vector space where the closer two points are, the more similar they are. This is in 3 dimensions but in reality the number of dimensions can range from 386 to 1536.</p> <p>There are a few different methods such as cosine similarity, dot product, Euclidean distance etc.</p>"},{"location":"concepts/vector_databases/#vector_databases_1","title":"Vector Databases","text":"<p>There are a number of exclusive vector databases that can be used for semantic search, but many traditonal databases have Vector search available as an option.</p> <p>Postgres has PGVector and PGAI for example that enable us to use just one database for hybrid search - both keyword and semantic search.</p> <p>This can be much faster as the semantic search is done in the database as SQL actions and there is not latency involved to connect to systems elsewhere.</p> <p>Indeed, Postgres claims to be as fast if not faster than many commercial Vector Database Services, https://www.timescale.com/blog/how-we-made-postgresql-as-fast-as-pinecone-for-vector-data/</p>"},{"location":"craig/contact/","title":"Contact me","text":"<p>Email: iwswordpress@gmail.com</p> <p>LinkedIn: Craig West</p>"},{"location":"craig/courses/","title":"Online courses","text":""},{"location":"craig/courses/#udemycom","title":"Udemy.com","text":"<p>The course Udemy Hooks and Plugins course has just been published and Udemy has a sale ever two weeks and the cost would be $20 USD approx.</p> <p></p> <p>I am currently developing two courses:</p> <ul> <li>Python - mock, patch and monkeypatch.</li> <li>PyTest Django Full Stack - a DB &lt;-&gt; E2E testing of a generic ecommerce store.</li> </ul> <p>The aim is to make them generic, ready to go templates, that also dive deeper into aspects of Python.</p> <p>I am of the opinion that as developers we do not need to reinvent the wheel - it has (almost) all been done before - and that we should be free to use our creativity to build great proucts.</p> <p>The type of course I would want...</p> <p></p>"},{"location":"craig/cv/","title":"CV","text":""},{"location":"craig/cv/#github_cv","title":"GitHub CV","text":"<p>I use GitHub to host a copy of my CV.</p> <p>Github CV</p>"},{"location":"craig/cv/#python_backend_and_test_automation_engineer","title":"Python Backend and Test Automation Engineer","text":"<ul> <li>Degree in Chemistry, Oxford University.</li> <li>Former A+ PC Technician, Microsoft Certified Systems Engineer and Microsoft Certified SQL Server DBA.</li> <li>Former Business Information Architect.</li> <li>Qualified Accountant Technician and business owner.</li> <li>Experience with REST APIs, GraphQL, React, Vue, Web Components, Node, Docker</li> <li>Talks and workshops given at WordCamps, MeetUps and NDC.</li> </ul>"},{"location":"craig/cv/#talks_and_workshops","title":"Talks and Workshops","text":""},{"location":"craig/cv/#python","title":"Python","text":"<ul> <li> <p>BrightonPy Feb 2025: AI as API in everyday Python apps - 60 minute talk and demo.</p> </li> <li> <p>Django Japan Congress Feb 2025 (online): Implementing Agentic AI solutions in Django from scratch - 45 minute talk</p> </li> <li> <p>Conf42 (online) Feb 2025: Implementing Agentic AI Solutions in Python from scratch - 50 minute talk.</p> </li> <li> <p>PyCon Ireland Nov 2024: Getting started with Pytest - 2 hr workshop.</p> </li> </ul> <p>Next...</p> <ul> <li>DjangoConEurope April 2025: Implementing Agentic AI solutions in Django from scratch - 90 minute workshop.</li> </ul>"},{"location":"craig/cv/#other","title":"Other","text":"<ul> <li> <p>TALK: Offline and instant websites, aka Progressive Web Apps - AsyncJS, Brighton, September 2021.</p> </li> <li> <p>LIGHTNING TALK: WordPress as a Micro Service to any framework - WordFest, July 2021.</p> </li> <li> <p>TALK: WP REST API and Web Components =&gt; 100% Internet - WordCamp Santa Clarita, July 2021.</p> </li> <li> <p>TALK: Web Components in WP, Gutenberg and as HTML plugins. - WordCamp North East Ohio May 2021.</p> </li> <li> <p>TALK: Leveraging the power or the WordPress REST API - WP Leeds April 2021</p> </li> <li> <p>WORKSHOP: WP REST API and you -&gt; Best Friends Forever workshop (90 mins) - WordCamp Greece April 2021. Code: https://github.com/iwswordpress/WordCampGreece</p> </li> <li> <p>TALK: Web Components as Micro Apps - NDC London, Jan 2021</p> </li> <li> <p>TALK: Unifying frameworks with Web Components - Brighton AsyncJS, Nov 2020.</p> </li> <li> <p>WORKSHOP: Progressive Web Apps Workshop (2hrs) - NDC Oslo June 2020 and a paid training workshop with NDC.</p> </li> <li> <p>WORKSHOP: Web Components Workshop (2hrs) - NDC Oslo June 2020 and a paid training workshope with NDC.</p> </li> <li> <p>WORKSHOP: Progressive Web Apps Workshop (2hrs) - Brighton WordUp June 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API with AJAX Forms and Pages - WordCamp Denver, June 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API with AJAX Forms and Pages - WordCamp Kent, Ohio May 2020.</p> </li> <li> <p>TALK: What is the WP REST API and how can I use it to make forms and pages that don\u2019t need to do be reloaded? - WordUp Brighton May 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API and AJAX Forms - WordCamp Geneva March 2020 [EVENT CANCELLED due to virus concerns :( </p> </li> <li> <p>TALK - WP-HTML: The marriage of WP and JS Frameworks for expansion, ubiquity and profit - WordCamp Vienna February 2020.</p> </li> <li> <p>WORKSHOP: WordPress REST API - WordCamp Vienna February 2020.</p> </li> <li> <p>TALK: Progressive Web Apps - Brighton WordUp November 2019.</p> </li> <li> <p>TALK: Decoupled WordPress (code along style) - WordCamp Dublin October 2019.</p> </li> <li> <p>TALK: JWT and Authentication - WPHooked London September 2019.</p> </li> <li> <p>TALK: Decoupled WordPress and WP Components - WordCamp Brighton August 2019.</p> </li> </ul>"},{"location":"craig/cv/#published_udemy_courses","title":"Published Udemy Courses","text":"<p>Udemy is a great learning platform and having sales at least once a month, courses can be purchased for ~ \u00a315/$15 USD.</p> <p>These have now been retired.</p> <ul> <li>WordPress REST API and AJAX Forms/Pages - DEMO https://www.youtube.com/watch?v=eubhbcGH_Ws&amp;t=6s (paid)</li> <li>Progressive Web Apps - DEMO https://www.youtube.com/watch?v=k_lHvNL0gkw (paid)</li> <li>WP-HTML: decoupling WordPress to any HTML platform using Web Components and the WP REST API. This also enables HTML plugins for non-WP Sites - https://www.udemy.com/course/powerful-html-pages-using-wordpress-component-architecture/ (free)</li> <li>Stylish Dynamic Web Forms with jQuery validation - https://www.udemy.com/course/ready-to-use-form-validation-templates-with-jquery/ (free)</li> </ul>"},{"location":"craig/cv/#youtube_courses_-_developer_to_developer_courses","title":"YouTube Courses - Developer to Developer courses","text":"<p>These are video courses that cover work through official documents to help other developers, learn in public and show prosepective employers not just what I know but how I learn and how I communicate technical matters to others.</p> <p>There are also some specific videos explaining solutions to set ups other developers may encounter.</p> <ul> <li> <p>HIGHLY-FUNCTIONAL-WEBCOMPONENTS: A video course based on the workshop I gave at NDC Oslo June 2020 - https://www.youtube.com/watch?v=QC-JTqQTv2k&amp;list=PLsszRSbzjyvkQwzrJobroRl7z7MfSlePa </p> </li> <li> <p>WP Plugin Boilerplate:  I havea video series to explain WP Plugin Boilerplate using a scaffolded out project that demonstrates the use of MySQL, wp_nonce, REST API, forms and how to redirect pages to plugin templates to make the plugin theme independent. https://www.youtube.com/watch?v=lJ9ktD4JOfs&amp;list=PLsszRSbzjyvn-RQr4dEjrgnTne2HcJKee</p> </li> </ul>"},{"location":"craig/cv/#volunteering","title":"Volunteering","text":"<p>I volunteer at Codebar.io in Brighton as well as some Community Kitchens.</p>"},{"location":"craig/cv/#outside_interests","title":"Outside interests","text":"<p>These include Community Kitchens, gym, occasional partner dancing and DIY.</p> <p></p>"},{"location":"craig/services/","title":"Services provided","text":""},{"location":"craig/services/#backend_pythonista_and_test_automation_engineer","title":"Backend Pythonista and Test Automation Engineer","text":""},{"location":"craig/services/#skillset","title":"Skillset","text":"<p>Primarily:</p> <ul> <li>Python</li> <li>PyTest</li> <li>Playwright</li> <li>Django</li> </ul> <p>Tools:</p> <p>I strive to dive deeper into these tools and see them as programming languages in their own right. DevOps seems to be an essential part of my work:</p> <ul> <li>Shell Scripting</li> <li>Git/GitHub Actions</li> <li>Docker</li> </ul>"},{"location":"craig/services/#engagement_style","title":"Engagement Style","text":"<p>I offer on-demand, freelance services starting from 1/2 day blocks.</p> <p>As and when you need it...</p> <p>Tech is a way of life for me not just a job and I strive to have enthusiasm and passion for the projects I work on. Professional fulfilment is paramount.</p>"},{"location":"craig/services/#eligibility","title":"Eligibility","text":"<ul> <li>UK National</li> <li>Fluent English</li> </ul>"},{"location":"craig/services/#on-sitehybrid","title":"On-site/Hybrid","text":"<p>I am based in Brighton and enjoy (local) on-site work as well as working from my home office.</p>"},{"location":"craig/services/#volunteer_coach","title":"Volunteer Coach","text":"<p>I am a volunteer coach with Codebar Brighton.</p>"},{"location":"craig/services/#youtube","title":"YouTube","text":"<p>I produce a large amount of content that is associated with a repo that enables 'out of the box' ease of use.</p> <p>If I find good videos without a repo, I often create a repo and my own video with reference to the source video. I have no commercial interest in this matter.</p> <p>My YouTube Channel</p>"},{"location":"craig/services/#outside_of_tech","title":"Outside of tech...","text":"<p>I enjoy working in community kitchens and love laughter, creating, doing and trying to work out why things are funny.</p> <p></p>"},{"location":"evaluation/domain_expert/","title":"Professional Evaluation","text":"<p>The technique of an (untested) LLm generating question/answer pairs to act as ground truths to then test an LLM seems illogical but current thinking states this is quite effective.</p> <p>At the end of the day, the final judge of the LLM is a human. </p> <p>A number of sets of questions can be made and the LLM evaluated against them by a domain expert.</p> <p>Whilst a numerical rating system is the immediate choice, there are many flaws with this.</p> <p>The domain expert can rate the repsonse for the following:</p> <ol> <li>Accuracy. Is the answer factually correct?</li> <li>Relevance. Is the answer relevant to the question?</li> <li>Completeness. Is the answer complete?</li> <li>Clarity. Is the answer clear?</li> </ol> <p>Only those that get a YES to ACCURACY and RELEVANCE will be considered as they are essential for a good response.</p> <p>This is a very effective way of evaluating the LLM and will help us to improve our system and will be modiofied in the future as needed.</p>"},{"location":"evaluation/domain_expert/#example","title":"Example","text":""},{"location":"evaluation/overview/","title":"Evaluation Overview","text":"<p>There are a number of ways we can evaluate the performance of the app. </p> <p>These use traditonal Natural Language Processing (NLP) and Machine Learning (ML) techniques.</p> <p>The main library we use, and this is more for the developers as we are developing different strategies, is RAGAS.</p>"},{"location":"evaluation/overview/#ragas_metrics","title":"Ragas metrics","text":""},{"location":"evaluation/overview/#context_precision","title":"Context Precision","text":"<p>This is a metric that measures the proportion of relevant chunks in the retrieved_contexts. It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.</p>"},{"location":"evaluation/overview/#context_recall","title":"Context Recall","text":"<p>Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out. In short, recall is about not missing anything important. Since it is about not missing anything, calculating context recall always requires a reference to compare against.</p>"},{"location":"evaluation/overview/#faithfulness","title":"Faithfulness","text":"<p>Faithfulness metric measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.</p> <p>The generated answer is regarded as faithful if all the claims made in the answer can be inferred from the given context. To calculate this, a set of claims from the generated answer is first identified. Then each of these claims is cross-checked with the given context to determine if it can be inferred from the context. </p>"},{"location":"evaluation/overview/#other_evaluations","title":"Other evaluations","text":"<p>There are many libraries for evaluating the perfromance of our app, like Giskard and Huggingface.</p>"},{"location":"evaluation/overview/#domain_expert_evaluation","title":"Domain Expert Evaluation","text":"<p>Ultimately, the real test is that of the domain expert.</p> <p>Here, we have a set of questions and answers and we want to evaluate the performance of the app.</p> <p>We can create a set of 'ground truths', questions and their correct answer set up by domain experts.</p> <p>We can then have our app generate answers and context to these questions for a domain expert to assess, not just the accuracy but also whether the context returned was relevant:</p> <p></p>"},{"location":"evaluation/rate_with_llm/","title":"LLM as judge","text":"<p>The technique of an (untested) LLm generating question/answer pairs to act as ground truths to then test an LLM seems illogical but current thinking states this is quite effective.</p> <p>This technique might be used by developers to improve the knowledge system as they work on it rather than getting human evaluations at every step.</p> <p>This is more of a development tool rather than final evalauation.</p> <p>Useful article</p>"},{"location":"evaluation/refine/","title":"Refining","text":"<p>Over time we will find what architecture works best for us.</p> <p>By recording users feedback, we will be able to improve our system as we will begin to see which architecture has the 'most votes' from our users.</p>"},{"location":"pipeline/01_gather/","title":"Gathering Docs","text":"<p>We need to create our own indexed library of documentrs so that we know what information we have.</p> <p>These will be collated for a specific medical domain.</p> <p>It would be helpful to have metadata about each document to help with filtering and correct retrieval.</p> <p>Data types can include:</p> <ul> <li>PDFs of articles.</li> <li>Tabular data within these articles.</li> <li>Images within these articles that have a text summary.</li> <li>Word, Powerpoint and Excel documents.</li> <li>Audio and video files to create transcripts to text as well video analysis of sequential images that are then summarised with text.</li> </ul> <p>MULTIMODAL</p>"},{"location":"pipeline/02_ingest/","title":"Ingestion","text":"<p>Once we have our data that has been cleaned and formatted, we need to ingest it into our knowledge base. Ingestion is the process of taking data and converting it into a format that can be used by our knowledge base.</p> <p>This means creating vector embeddings for each chunk, (which is termed a document and many documents make a paper/pdf) and then indexing them into a vector database.</p> <ul> <li>https://www.youtube.com/watch?v=ymON0qXbbdw</li> <li>scispacy </li> <li>medcat</li> </ul>"},{"location":"pipeline/02_ingest/#prepocessing","title":"Prepocessing","text":"<p>We can use ML NLP to create additional metadata for each document.</p> <p>This will enable Hybrid Search where we use both semantic and traditional lexical searches.</p>"},{"location":"pipeline/02_ingest/#tokenization","title":"Tokenization","text":"<p>We can convert the document to tokens that form a set of tags to help with filtering queries.</p>"},{"location":"pipeline/02_ingest/#named_entity_recognition","title":"Named Entity Recognition","text":"<p>We can use MedNER to extract named entities from each document. This can help with traditional searches/filtering as well as enable GraphDB creation.</p>"},{"location":"pipeline/02_ingest/#summarization","title":"Summarization","text":"<p>For documents with a certain amount of content, we can do NLP Summarization, vecorizing the summary and indexing it into the vector database.</p> <p>This will also enable article summarization for users and we can sue the LLM to combine summarising the document as a whole with the summary of all the section summaries.</p>"},{"location":"pipeline/02_ingest/#create_questions","title":"Create questions","text":"<p>Given a document (chunk), we can use the LLM to create questions that can be used to improve retrieval. \"What questions does this document answer?\"</p> <p>This is a technique of RAG but rather than do this with each search, we can as a background task do this with each document.</p> <p>We then have the options to add this strategy if needed.</p>"},{"location":"pipeline/02_ingest/#postgresql_fts","title":"Postgresql FTS","text":"<p>We can use Postgresql FTS to create a full text search engine for our knowledge base.</p> <p>This more than just <code>LIKE 'word*</code> and for many searches that involve terms much better than semantic search where semantic search may fail.</p> <p>This is called Hybrid Search.</p> <p>https://bigmachine.io/2022/06/12/creating-a-full-text-search-engine-in-postgresql-2022/</p> <p></p>"},{"location":"pipeline/03_improve_question/","title":"Improve question","text":"<p>We can process the query using a pre-query to increase the quality of retrieval.</p> <p>As in human life, better questions lead to better answers.</p>"},{"location":"pipeline/03_improve_question/#multi_query","title":"Multi Query","text":"<p>We get the LLM to make N versions of the user query and then get all the unique documents for them.</p> <p>If we have done some of the preprocessing mentioned previously in the \"Ingest and NLP\" phase, we can then have a better hit rate as we will have many variations of the user query compared to the many documents in the knowledge base that have had preprocessing applied to them.</p>"},{"location":"pipeline/03_improve_question/#hyde","title":"HyDE","text":"<p>Hypothetical document embeddings is a technique that creates a theoretical document when responding to a query, as opposed to using the query and its computed vector to directly seek in the vector database.</p>"},{"location":"pipeline/04_retrieve_docs/","title":"Query","text":"<p>With our retrieved documents and the user question, we can now add them to one of Promtpt Templates to send to the LLM to generate a response.</p> <p>We will have a number of Prompt Templates and when the user finally rates the response, we will be able to record which template, as well as all the other strategy parameters, enabling us to refine our system.</p> <p>This is where PROMPT ENGINEERING comes into play.</p> <p>We can ask the LLM to improve on the response by resubmitting the query with a request to improve it.</p>"},{"location":"pipeline/05_rank_and_filter/","title":"Ranking","text":"<p>We can use an AI Grading Agent to grade the quality of retrieved documents, check the response for hallucinations and other issues.</p> <p>This can be used at any stage in the pipeline.</p> <p>It is the general pricnciple of adding the previous output to the message history, making it in effect a brand new query with a new query to the LLM.</p>"},{"location":"pipeline/06_generate/","title":"Generation","text":"<p>Finally, once all thge checks have been passed the LLM can pass the response to the user ready for user evaluation and feedback.</p> <p>We can use agents that create specific reports based on the instructions we give it to generate reports for a user.</p> <p>An example of this is Langgraph's report-mAIstro that uses parallel and serial processes to generate sections for a report based on the user's question:</p> <p></p> <p>All we need modify is the prompt template:</p> <pre><code>This report type focuses on comparative analysis.\n\nThe report structure should include:\n1. Introduction (no research needed)\n   - Brief overview of the topic area\n   - Context for the comparison\n\n2. Main Body Sections:\n   - One dedicated section for EACH offering being compared in the user-provided list\n   - Each section should examine:\n     - Core Features (bulleted list)\n     - Architecture &amp; Implementation (2-3 sentences)\n     - One example use case (2-3 sentences)\n\n3. No Main Body Sections other than the ones dedicated to each offering in the user-provided list\n\n4. Conclusion with Comparison Table (no research needed)\n   - Structured comparison table that:\n     * Compares all offerings from the user-provided list across key dimensions\n     * Highlights relative strengths and weaknesses\n   - Final recommendations\n</code></pre> <p>meaning that a non-tech business analyst, for example, can develop a report on any topic area in any style by supplying a text file to the developers or submit via a form for example.</p>"},{"location":"pipeline/07_user_rate_and_cache/","title":"User ratings","text":"<p>Once a user gets a final response, the user will have the option to rate the response. </p> <p>We will record if they give it a thumbs up or down and they can also add thier own comments/additions to the response.</p> <p>These are all saved in the database so that later when another user asks a question we can check if a similar one with a positive rating exists. This can be fouind very quickly and offered to the user with the option of requesting further information.</p> <p>We can see if other users adding ratings to cached results given to them is effective. In this way we get a community rating of answers to questions.</p> <p>When a cached response is offered to a user, they could see statistics of what others think of this answer.</p> <p>This is an established techincal feature of many article sites.</p> <p>A clever UI could make all this feedback data and additonal information clean and effective rather than cluttering up the page.</p> <p></p>"}]}